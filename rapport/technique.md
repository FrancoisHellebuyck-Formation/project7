# Rapport technique ‚Äì Assistant intelligent de recommandation d‚Äô√©v√©nements culturels

## 1. Objectifs du projet
### Contexte : 

- Puls-Events est une entreprise technologique innovante sp√©cialis√©e dans le d√©veloppement d'une plateforme de recommandations culturelles personnalis√©es.

- Pour am√©liorer l'exp√©rience utilisateur et r√©pondre √† l'√©volution des attentes num√©riques, Puls-Events souhaite int√©grer un assistant intelligent capable de g√©rer les requ√™tes des utilisateurs en temps r√©el.

### Probl√©matique :

- Un syst√®me RAG (Retrieval-Augmented Generation) r√©pond aux besoins m√©tier de Puls-Events en r√©solvant la probl√©matique centrale de l'acc√®s pr√©cis et fiable √† l'information √©v√©nementielle par un chatbot.

- Le RAG surmonte les limites des mod√®les de langage classiques (LLMs) pour fournir une solution √† la fois performante et √©conomiquement viable.

### Objectif du POC :

- L'objectif du POC pour Puls-Events est de d√©montrer de mani√®re concr√®te et mesurable que la technologie RAG (Retrieval-Augmented Generation), en utilisant LangChain, Mistral et FAISS, est la solution optimale pour alimenter le futur chatbot d'√©v√©nements culturels.

- Cet objectif se d√©cline en trois axes principaux : 
    - la faisabilit√© technique, 
    - la valeur m√©tier 
    - et la performance.

1. D√©montrer la Faisabilit√© Technique üõ†Ô∏è

Il s'agit de prouver que l'int√©gration des composants cl√©s est fonctionnelle et stable, menant √† une solution pr√™te pour l'industrialisation.

Int√©gration du Pipeline Complet : Prouver la capacit√© √† orchestrer le flux de donn√©es de bout en bout : de l'extraction des donn√©es d'√©v√©nements r√©cents via l'API Open Agenda, √† leur transformation en embeddings, leur stockage dans l'index FAISS, et leur utilisation par le LLM Mistral via LangChain pour la g√©n√©ration de la r√©ponse.

Portabilit√© et D√©ploiement : Valider la capacit√© √† livrer un syst√®me standardis√© et reproductible gr√¢ce √† la conteneurisation Docker et √† l'exposition via une API REST (FastAPI).

2. D√©montrer la Valeur M√©tier (Pertinence) ‚ú®

L'objectif est de s'assurer que le syst√®me r√©pond directement aux besoins de l'utilisateur final et de l'entreprise Puls-Events.

V√©racit√© des R√©ponses : D√©montrer que le RAG √©limine les "hallucinations" en basant syst√©matiquement les r√©ponses sur le contexte factuel et √† jour des √©v√©nements (dates, lieux, artistes, genres) extrait d'Open Agenda. C'est la validation de la fiabilit√© de l'information.

Exp√©rience Utilisateur Am√©lior√©e : Prouver que le chatbot peut g√©rer et r√©pondre avec fluidit√© √† une grande vari√©t√© de questions en langage naturel, y compris les requ√™tes s√©mantiques complexes (e.g., "Je cherche quelque chose de familial le week-end prochain") bas√©es sur le jeu de test annot√©.

Efficacit√© Op√©rationnelle : Montrer que ce syst√®me est plus rentable et plus rapide √† actualiser qu'une approche de fine-tuning du LLM, car seule la base vectorielle a besoin d'√™tre mise √† jour avec les nouveaux √©v√©nements.

3. D√©montrer la Performance (Qualit√© et Rapidit√©) ‚ö°

Il faut quantifier l'efficacit√© du syst√®me √† la fois sur la recherche et la g√©n√©ration.

Performance du Retrieval : Mesurer l'efficacit√© de FAISS √† remonter les fragments de texte pertinents. Le Hit Rate (pourcentage de fois o√π le bon fragment est dans les top-k r√©sultats) est la m√©trique cl√© pour valider que la bonne information est trouv√©e.

Qualit√© de la G√©n√©ration : Mesurer la fid√©lit√© (faithfulness) et la pertinence de la r√©ponse g√©n√©r√©e par Mistral par rapport au contexte fourni. La r√©ponse doit √™tre bien r√©dig√©e, concise et r√©pondre directement √† la question de l'utilisateur.

Latence (Temps de R√©ponse) : S'assurer que le syst√®me complet (API + RAG) offre un temps de r√©ponse acceptable pour une exp√©rience utilisateur fluide (cible typique : quelques secondes ou moins).


- P√©rim√®tre : Zone g√©ographique cibl√©e, p√©riode d‚Äô√©v√©nements, donn√©es utilis√©es.

## Architecture du syst√®me
### Sch√©ma global :

![Sch√©ma d'architecture](./Architecture.png)

### Donn√©es entrantes (API Open Agenda)

**Source de donn√©es :** API Open Agenda v2 (https://api.openagenda.com/v2)

**Endpoints utilis√©s :**
- `/agendas` : R√©cup√©ration des agendas culturels officiels par r√©gion
- `/agendas/{uid}/events` : R√©cup√©ration des √©v√©nements pour chaque agenda

**Param√®tres de collecte :**
- **R√©gion cibl√©e** : Occitanie (configurable via `OA_REGION` dans .env)
- **Pagination** : Curseur `after[]` avec taille de page de 100 √©v√©nements (`OA_PAGE_SIZE=100`)
- **Filtrage temporel** :
  - Agendas : `updatedAt >= date` (derni√®re ex√©cution ou 1 an par d√©faut)
  - √âv√©nements : `createdAt >= date` OU `updatedAt >= date` (mode UPDATE)

**Donn√©es extraites par √©v√©nement :**
- **M√©tadonn√©es** : uid, title, description, slug
- **Temporalit√©** : timings (date_debut, date_fin), createdAt, updatedAt
- **Localisation** : location (coordinates, name, address, city, region)
- **Classification** : keywords, categories
- **Relations** : agendaUid (lien avec l'agenda parent)

**Stockage interm√©diaire :**
- **Base MongoDB** : Collections `agendas` et `events`
- **Strat√©gie upsert** : √âvite les doublons gr√¢ce √† des cl√©s uniques (uid pour agendas, (uid, agendaUid) pour events)
- **D√©doublonnement** : Script de nettoyage pour √©liminer les √©v√©nements dupliqu√©s par uid

**Mise √† jour incr√©mentale :**
- Pipeline de mise √† jour qui sauvegarde les collections existantes (`_update_YYYYMMDD_HHMMSS`)
- R√©cup√©ration s√©lective des agendas/√©v√©nements modifi√©s depuis la derni√®re ex√©cution
- Tracking des ex√©cutions dans la collection `last_update` avec m√©tadonn√©es compl√®tes

### Pr√©traitement / embeddings / base vectorielle

**Pipeline de traitement (src/chunks/chunks_document.py) :**

1. **Formatage des documents**
   - Conversion des √©v√©nements MongoDB en texte structur√©
   - Format : `Titre: {title}\nDates: {date_debut} - {date_fin}\nDescription: {description}\nLieu: {locationName}\nMots-cl√©s: {keywords}`

2. **Extraction des m√©tadonn√©es**
   - Champs conserv√©s : event_id, title, city, date_debut, date_fin, location (coordonn√©es GPS), region, keywords

3. **Chunking (LangChain RecursiveCharacterTextSplitter)**
   - **Taille des chunks** : 500 caract√®res (configurable via `CHUNK_SIZE`)
   - **Overlap** : 100 caract√®res (configurable via `CHUNK_OVERLAP`)
   - **Raison** : √âquilibre entre contexte suffisant et pr√©cision de la recherche
   - **Sortie** : Objets LangChain `Document` avec contenu + m√©tadonn√©es

**G√©n√©ration des embeddings (src/embeddings/embeddings.py) :**

- **Mod√®le** : `intfloat/multilingual-e5-large` (HuggingFace Transformers)
- **Dimensionnalit√©** : 1024 dimensions
- **Multilingue** : Support de 100+ langues incluant le fran√ßais
- **Local** : Pas d'API externe, inf√©rence locale (pas de cl√© API requise)
- **Strat√©gie** :
  - Average pooling avec masque d'attention
  - Pr√©fixes : "passage:" pour documents, "query:" pour requ√™tes
  - Normalisation L2 pour similarit√© cosinus optimale
- **Performance** :
  - D√©tection automatique du device (CUDA, MPS, CPU)
  - Batch processing (taille de batch configurable, d√©faut : 32)
  - ~50-100 chunks/seconde sur Apple Silicon (MPS)

**Construction de la base vectorielle (src/vectors/vectors.py) :**

- **Biblioth√®que** : FAISS (Facebook AI Similarity Search)
- **Type d'index** : FAISS avec LangChain wrapper
- **Persistance** :
  - Format : Fichiers binaires FAISS + pickle pour m√©tadonn√©es
  - Chemin : `data/faiss_index/` (configurable via `FAISS_INDEX_PATH`)
  - Sauvegarde : `index.faiss` + `index.pkl`
- **M√©tadonn√©es stock√©es** : Toutes les m√©tadonn√©es extraites sont conserv√©es avec chaque vecteur
- **Op√©rations support√©es** :
  - Cr√©ation d'index
  - Chargement d'index existant
  - Ajout de documents
  - Recherche par similarit√© (similarity_search_with_score)
  - Suppression d'index
  - Statistiques (nombre de vecteurs, dimension)

**Statistiques actuelles :**
- ~28,962 vecteurs index√©s (exemple du d√©veloppement)
- Dimension : 1024
- Couvre tous les √©v√©nements culturels d'Occitanie r√©cents

### Int√©gration LLM avec LangChain

**Mod√®le LLM s√©lectionn√© :**
- **Fournisseur** : Mistral AI
- **Mod√®le** : `mistral-small-latest` (configurable via `MISTRAL_MODEL`)
- **Raisons du choix** :
  - Excellence sur le fran√ßais
  - Rapport qualit√©/co√ªt optimal
  - Latence faible
  - Compatibilit√© native avec LangChain
  - API simple et fiable

**Architecture RAG (Retrieval-Augmented Generation) :**

1. **Recherche s√©mantique (Retrieval)**
   - Query embedding avec le m√™me mod√®le E5
   - Recherche FAISS des top-k documents similaires (k=5 par d√©faut)
   - R√©cup√©ration du contenu + m√©tadonn√©es + scores de similarit√©

2. **Enrichissement du contexte**
   - Formatage des documents r√©cup√©r√©s en contexte structur√©
   - Inclusion des m√©tadonn√©es pertinentes (titre, lieu, dates)
   - Limitation du contexte pour √©viter le d√©passement de tokens

3. **G√©n√©ration de r√©ponse**
   - **Prompt syst√®me** : Charg√© depuis `src/chat/ps.md` (Puls-Events persona)
   - **Directives** :
     - R√©ponses bas√©es uniquement sur le contexte fourni
     - Champ d'application : Occitanie et √©v√©nements culturels
     - Ton : Enthousiaste, accueillant, clair et concis
     - Gestion de l'ambigu√Øt√© et des questions hors-sujet
   - **Prompt utilisateur enrichi** : Question + contexte RAG
   - **Appel Mistral AI** : Via `mistral_client.chat.complete()`
   - **Retour** : R√©ponse + contexte utilis√© + statistiques tokens

**Impl√©mentation LangChain :**
- **Custom Embeddings** : Classe `E5Embeddings(Embeddings)` compatible LangChain
- **Vector Store** : Wrapper FAISS de LangChain
- **Retrieval** : `vector_store.similarity_search_with_score(query, k=k)`
- **Messages** : `SystemMessage` + `UserMessage` pour Mistral AI

**Gestion de la qualit√© :**
- Syst√®me de scoring de similarit√© pour filtrer les r√©sultats peu pertinents
- Limitation du nombre de documents contextuels (√©vite la surcharge)
- Tracking des tokens utilis√©s (prompt + completion + total)
- Fallback gracieux si pas de contexte pertinent trouv√©

### Exposition via API

**Framework** : FastAPI 0.120.1+

**Architecture de l'API (src/api/main.py) :**

**Endpoints principaux :**

1. **GET /** - Point d'entr√©e
   - Liste tous les endpoints disponibles
   - Version de l'API

2. **GET /health** - Health check
   - Statut : ok | degraded
   - √âtat des composants : vector_store, embeddings_model, mistral_client
   - Permet le monitoring

3. **GET /stats** - Statistiques du vector store
   - Nombre de vecteurs index√©s
   - Dimension des vecteurs
   - Chemin de l'index

4. **POST /search** - Recherche s√©mantique pure
   - **Entr√©e** : `{"query": "...", "k": 5}`
   - **Sortie** : R√©sultats avec scores, titres, contenus, m√©tadonn√©es
   - **Validation** : query non vide, k entre 1 et 100

5. **POST /ask** - Question-r√©ponse avec RAG + Mistral AI
   - **Entr√©e** : `{"question": "...", "k": 5, "system_prompt": "..." (optionnel)}`
   - **Processus** :
     1. Recherche s√©mantique (top-k documents)
     2. Enrichissement du prompt avec contexte
     3. Appel Mistral AI
     4. Retour de la r√©ponse
   - **Sortie** : `{"question": "...", "answer": "...", "context_used": [...], "tokens_used": {...}}`

6. **POST /rebuild** - Reconstruction incr√©mentale de l'index FAISS
   - Lance `update_pipeline.py` en arri√®re-plan
   - V√©rification pr√©alable : nouveaux √©v√©nements pr√©sents ?
   - **Workflow** :
     1. R√©cup√®re la date de derni√®re ex√©cution
     2. Compte les nouveaux √©v√©nements MongoDB
     3. Si aucun : annule avec statut "warning"
     4. Sinon : lance le pipeline complet
     5. Recharge automatiquement l'index en m√©moire
   - **Statuts** : started | running | success | success_with_warning | warning | error
   - Protection anti-concurrence (un seul rebuild √† la fois)

7. **GET /rebuild/status** - Suivi du rebuild
   - Statut actuel et d√©tails
   - Date de derni√®re mise √† jour
   - Timestamps de d√©marrage

**Fonctionnalit√©s techniques :**

- **CORS** : Configur√© pour autoriser les requ√™tes cross-origin
- **Startup event** : Chargement automatique du vector store + embeddings + Mistral client au d√©marrage
- **Background tasks** : Ex√©cution asynchrone du rebuild sans bloquer l'API
- **Rechargement automatique** : Nouvel index FAISS charg√© en m√©moire apr√®s rebuild r√©ussi
- **Auto-documentation** : Swagger UI accessible √† `/docs`
- **Gestion d'erreurs** : HTTPException avec codes appropri√©s (422, 503, 500)
- **Logging** : Logs d√©taill√©s de toutes les op√©rations
- **Hot-reload** : Activ√© en mode d√©veloppement

**Format des r√©ponses :**
- JSON structur√© avec mod√®les Pydantic (validation automatique)
- Codes HTTP standards
- Messages d'erreur explicites

**D√©ploiement :**
- **D√©veloppement** : `uvicorn` avec hot-reload (`make run-api`)
- **Production** : Docker + docker-compose
- **Port** : 8000 (configurable)
- **Host** : 0.0.0.0

**Tests unitaires :**
- 10 tests passants couvrant tous les endpoints principaux
- Mocking complet des d√©pendances (MongoDB, FAISS, Mistral)
- Pytest avec support async
- Commandes : `make test` | `make test-cov`

### Technologies utilis√©es :

**Backend et orchestration :**
- **Python 3.13+** : Langage principal
- **FastAPI 0.120.1+** : Framework web moderne et rapide
- **Uvicorn** : Serveur ASGI avec hot-reload
- **LangChain 1.0.2+** : Framework d'orchestration LLM
  - `langchain-community` : Int√©grations communautaires
  - `langchain-mistralai` : Connecteur Mistral AI
  - `langchain-text-splitters` : Chunking de documents
- **python-dotenv** : Gestion des variables d'environnement

**LLM et embeddings :**
- **Mistral AI API (mistralai 1.9.11+)** : G√©n√©ration de r√©ponses
  - Mod√®le : `mistral-small-latest`
  - SystemMessage/UserMessage pour le chat
- **HuggingFace Transformers 4.57.1+** : Mod√®les NLP locaux
  - `intfloat/multilingual-e5-large` : Embeddings multilingues (1024 dim)
- **PyTorch 2.9.0+** : Backend pour les transformers
  - Support CUDA/MPS/CPU

**Base de donn√©es et vectorielle :**
- **MongoDB (PyMongo 4.15.3+ / Motor 3.7.1+)** : Stockage des √©v√©nements
  - Collections : `agendas`, `events`, `last_update`
  - Op√©rations bulk avec upsert
- **FAISS (faiss-cpu 1.12.0+)** : Recherche vectorielle
  - Index persistant sur disque
  - Recherche par similarit√© cosinus

**Scraping et API :**
- **Requests 2.32.5+** : Appels HTTP vers Open Agenda API
- **BeautifulSoup4 (bs4 0.0.2+)** : Parsing HTML si n√©cessaire

**Validation et mod√®les de donn√©es :**
- **Pydantic 2.12.3+** : Validation des donn√©es API
  - Mod√®les pour requ√™tes/r√©ponses FastAPI

**Outils de d√©veloppement :**
- **pytest 8.3.0+** : Tests unitaires
  - `pytest-asyncio 0.24.0+` : Support async
  - `pytest-cov 6.0.0+` : Couverture de code
  - `httpx 0.28.0+` : Client HTTP async pour tests
- **flake8 7.3.0+** : Linter Python
- **uv** : Gestionnaire de d√©pendances rapide
- **Make** : Orchestration des commandes

**Conteneurisation et d√©ploiement :**
- **Docker** : Conteneurisation de l'application
- **docker-compose** : Orchestration multi-conteneurs
  - Service MongoDB
  - Service API FastAPI

**Utilitaires :**
- **NumPy 2.3.4+** : Calculs matriciels
- **pathlib** : Manipulation de chemins
- **asyncio** : Programmation asynchrone
- **logging** : Journalisation applicative

**Configuration syst√®me :**
- **macOS fix** : `KMP_DUPLICATE_LIB_OK=TRUE` pour OpenMP
- **Environnement** : Fichier `.env` pour la configuration
- **Makefile** : Commandes standardis√©es (`make install`, `make run-all`, etc.)


3. Pr√©paration et vectorisation des donn√©es
Source de donn√©es : API Open Agenda (param√®tres utilis√©s, filtres appliqu√©s)
Nettoyage : Exemples d‚Äôanomalies corrig√©es, m√©thodes utilis√©es
Chunking : Raison du d√©coupage, taille choisie
Embedding :
Mod√®le utilis√© (ex. : Mistral embedding API)
Dimensionnalit√©, logique de batch, format des vecteurs

4. Choix du mod√®le NLP
Mod√®le s√©lectionn√© :
Pourquoi ce mod√®le ? (Crit√®res : co√ªt, qualit√©, compatibilit√© LangChain‚Ä¶)
Prompting (si utilis√©) : Prompt de base / structure
Limites du mod√®le : 

5. Construction de la base vectorielle
Faiss utilis√© :
Strat√©gie de persistance : 
Format de sauvegarde ?
nommage ?
M√©tadonn√©es associ√©es : 
Ce qui est conserv√© pour chaque document

6. API et endpoints expos√©s
Framework utilis√© : FastAPI / Flask
Endpoints cl√©s :
/ask : question utilisateur, r√©ponse du syst√®me
/rebuild : reconstruction de l‚Äôindex (si besoin)
Format des requ√™tes/r√©ponses
Exemple d‚Äôappel API : avec curl ou code Python
Tests effectu√©s et document√©s
Gestion des erreurs / limitations

7. √âvaluation du syst√®me
Jeu de test annot√© :
Nombre d‚Äôexemples
M√©thode d‚Äôannotation
M√©triques d‚Äô√©valuation :
Exemple : similarit√© s√©mantique, taux de couverture des r√©ponses, score de satisfaction (subjectif)
R√©sultats obtenus :
Analyse quantitative (scores globaux)
Analyse qualitative (exemples de bonnes/mauvaises r√©ponses)

8. Recommandations et perspectives
Ce qui fonctionne bien
Limites du POC :
Volum√©trie, performance, co√ªt, couverture th√©matique ?
Am√©liorations possibles :
Ajout de‚Ä¶Am√©lioration de..
Passage en production via‚Ä¶

9.  Organisation du d√©p√¥t GitHub
Arborescence du d√©p√¥t (fichiers cl√©s, scripts, dossiers)
Explication rapide de chaque r√©pertoire

10. Annexes (exemples)
Extraits du jeu de test annot√©
Prompt utilis√© (si sp√©cifique)
Extraits de logs ou exemples de r√©ponse JSON
