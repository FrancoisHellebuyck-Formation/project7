"""
API FastAPI pour l'interrogation du vector store FAISS.

Cette API permet d'effectuer des recherches s√©mantiques sur les √©v√©nements culturels
en utilisant le vector store FAISS pr√©-calcul√©.
"""

import logging
import os
import asyncio
from datetime import datetime

from dotenv import load_dotenv
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from mistralai import Mistral, UserMessage, SystemMessage

from embeddings.embeddings import get_embeddings_model
from vectors.vectors import load_vector_store, get_vector_store_stats
from api.models import (
    SearchQuery,
    SearchResult,
    SearchResponse,
    AskQuery,
    AskResponse,
    StatsResponse,
    HealthResponse,
    RebuildResponse,
)

# Configuration du logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Chargement des variables d'environnement
load_dotenv()

# Configuration
# Note: Le chemin doit √™tre absolu ou relatif au r√©pertoire racine du projet
_faiss_index_path = os.getenv("FAISS_INDEX_PATH", "data/faiss_index")
# Si le chemin n'est pas absolu, le rendre relatif au r√©pertoire racine du projet
# __file__ est src/api/main.py, donc on remonte 3 niveaux pour arriver √† la racine
if not os.path.isabs(_faiss_index_path):
    project_root = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
    FAISS_INDEX_PATH = os.path.join(project_root, _faiss_index_path)
else:
    FAISS_INDEX_PATH = _faiss_index_path

EMBEDDINGS_MODEL = os.getenv("EMBEDDINGS_MODEL", "intfloat/multilingual-e5-large")
EMBEDDINGS_DEVICE = os.getenv("EMBEDDINGS_DEVICE") or None

# Configuration Mistral AI
MISTRAL_API_KEY = os.getenv("MISTRAL_API_KEY")
MISTRAL_MODEL = os.getenv("MISTRAL_MODEL", "mistral-small-latest")
RAG_TOP_K = int(os.getenv("RAG_TOP_K", "5"))

# Initialisation de l'application FastAPI
app = FastAPI(
    title="API de recherche d'√©v√©nements culturels",
    description="API pour effectuer des recherches s√©mantiques sur les √©v√©nements culturels de la r√©gion Occitanie",
    version="1.0.0",
)

# Configuration CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],  # En production, sp√©cifier les domaines autoris√©s
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Variables globales pour le vector store et le mod√®le d'embeddings
vector_store = None
embeddings_model = None
mistral_client = None
default_system_prompt = None

# Variables pour suivre l'√©tat du rebuild
rebuild_in_progress = False
rebuild_status = {
    "status": "idle",
    "message": "Aucun rebuild en cours",
    "started_at": None,
    "last_update_date": None
}


def load_system_prompt(file_path: str) -> str:
    """
    Charge le prompt syst√®me depuis un fichier markdown.

    Args:
        file_path: Chemin vers le fichier .md contenant le prompt syst√®me

    Returns:
        Contenu du fichier comme cha√Æne de caract√®res

    Raises:
        FileNotFoundError: Si le fichier n'existe pas
    """
    try:
        with open(file_path, "r", encoding="utf-8") as f:
            content = f.read()
            logger.info(f"‚úì Prompt syst√®me charg√© depuis: {file_path}")
            return content
    except FileNotFoundError:
        logger.error(f"‚ùå Fichier de prompt syst√®me introuvable: {file_path}")
        raise
    except Exception as e:
        logger.error(f"‚ùå Erreur lors du chargement du prompt syst√®me: {e}")
        raise


@app.on_event("startup")
async def startup_event():
    """Initialise le vector store et le mod√®le d'embeddings au d√©marrage."""
    global vector_store, embeddings_model, mistral_client, default_system_prompt

    logger.info("=" * 70)
    logger.info("D√âMARRAGE DE L'API DE RECHERCHE")
    logger.info("=" * 70)

    try:
        # Chargement du mod√®le d'embeddings
        logger.info("Chargement du mod√®le d'embeddings...")
        embeddings_model = get_embeddings_model(
            model_id=EMBEDDINGS_MODEL,
            device=EMBEDDINGS_DEVICE
        )
        logger.info("‚úì Mod√®le d'embeddings charg√©")

        # Chargement du vector store
        logger.info(f"Chargement du vector store depuis: {FAISS_INDEX_PATH}")
        vector_store = load_vector_store(
            load_path=FAISS_INDEX_PATH,
            embeddings=embeddings_model
        )

        # Affichage des statistiques
        stats = get_vector_store_stats(vector_store)
        logger.info("‚úì Vector store charg√©")
        logger.info(f"  - Nombre de vecteurs: {stats['num_vectors']:,}")
        logger.info(f"  - Dimension: {stats['dimension']}")

        # Initialisation du client Mistral AI (si cl√© API disponible)
        if MISTRAL_API_KEY:
            logger.info("Initialisation du client Mistral AI...")
            mistral_client = Mistral(api_key=MISTRAL_API_KEY)
            logger.info("‚úì Client Mistral AI initialis√©")

            # Chargement du prompt syst√®me depuis le fichier ps.md
            # Le fichier ps.md est dans src/chat/, et ce fichier est src/api/main.py
            # Donc on remonte d'un niveau puis on va dans chat/
            prompt_file_path = os.path.join(
                os.path.dirname(os.path.dirname(__file__)),
                "chat",
                "ps.md"
            )
            logger.info(f"Chargement du prompt syst√®me depuis: {prompt_file_path}")
            default_system_prompt = load_system_prompt(prompt_file_path)
        else:
            logger.warning("‚ö†Ô∏è  MISTRAL_API_KEY non configur√©e - endpoint /ask d√©sactiv√©")

        logger.info("=" * 70)
        logger.info("‚úì API PR√äTE √Ä RECEVOIR DES REQU√äTES")
        logger.info("=" * 70)

    except Exception as e:
        logger.error(f"‚ùå Erreur lors du chargement: {e}")
        raise


@app.get("/", response_model=dict)
async def root():
    """Point d'entr√©e racine de l'API."""
    return {
        "message": "API de recherche d'√©v√©nements culturels",
        "version": "1.0.0",
        "endpoints": {
            "search": "/search",
            "ask": "/ask",
            "stats": "/stats",
            "health": "/health",
            "rebuild": "/rebuild",
            "rebuild_status": "/rebuild/status",
            "docs": "/docs"
        }
    }


@app.get("/health", response_model=HealthResponse)
async def health_check():
    """V√©rifie l'√©tat de sant√© de l'API."""
    return HealthResponse(
        status="ok" if vector_store and embeddings_model else "degraded",
        vector_store_loaded=vector_store is not None,
        embeddings_model_loaded=embeddings_model is not None,
        mistral_client_loaded=mistral_client is not None
    )


@app.get("/stats", response_model=StatsResponse)
async def get_stats():
    """Retourne les statistiques du vector store."""
    if not vector_store:
        raise HTTPException(status_code=503, detail="Vector store non charg√©")

    try:
        stats = get_vector_store_stats(vector_store)
        return StatsResponse(
            num_vectors=stats["num_vectors"],
            dimension=stats["dimension"],
            index_path=FAISS_INDEX_PATH
        )
    except Exception as e:
        logger.error(f"Erreur lors de la r√©cup√©ration des stats: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/search", response_model=SearchResponse)
async def search(query: SearchQuery):
    """
    Effectue une recherche s√©mantique sur les √©v√©nements culturels (m√©thode POST).

    Args:
        query: Objet contenant la requ√™te et le nombre de r√©sultats souhait√©s

    Returns:
        Liste des r√©sultats de recherche avec scores et m√©tadonn√©es
    """
    if not vector_store or not embeddings_model:
        raise HTTPException(status_code=503, detail="Vector store ou mod√®le d'embeddings non charg√©")

    try:
        logger.info(f"Recherche: '{query.query}' (k={query.k})")

        # Recherche dans le vector store
        results = vector_store.similarity_search_with_score(query.query, k=query.k)

        # Formatage des r√©sultats
        formatted_results = []
        for doc, score in results:
            result = SearchResult(
                score=float(score),
                title=doc.metadata.get("title", "Sans titre"),
                content=doc.page_content,
                location=doc.metadata.get("location"),
                metadata=doc.metadata
            )
            formatted_results.append(result)

        logger.info(f"‚úì {len(formatted_results)} r√©sultats trouv√©s")

        return SearchResponse(
            query=query.query,
            results=formatted_results,
            total_results=len(formatted_results)
        )

    except Exception as e:
        logger.error(f"Erreur lors de la recherche: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.post("/ask", response_model=AskResponse)
async def ask_question(query: AskQuery):
    """
    R√©pond √† une question en utilisant RAG + Mistral AI.

    Cette endpoint combine la recherche s√©mantique (RAG) avec l'API Mistral AI
    pour fournir des r√©ponses contextuelles bas√©es sur les √©v√©nements culturels.

    Workflow:
    1. Recherche s√©mantique dans le vector store (top-k r√©sultats)
    2. Formatage du contexte avec les √©v√©nements trouv√©s
    3. Enrichissement du prompt utilisateur
    4. Appel √† Mistral AI pour g√©n√©rer la r√©ponse
    5. Retour de la r√©ponse avec contexte et statistiques

    Args:
        query: Objet contenant la question et les param√®tres

    Returns:
        R√©ponse g√©n√©r√©e avec contexte et statistiques d'utilisation
    """
    if not vector_store or not embeddings_model:
        raise HTTPException(
            status_code=503,
            detail="Vector store ou mod√®le d'embeddings non charg√©"
        )

    if not mistral_client:
        raise HTTPException(
            status_code=503,
            detail="Client Mistral AI non initialis√©. V√©rifiez MISTRAL_API_KEY dans .env"
        )

    try:
        logger.info(f"Question re√ßue: '{query.question}' (k={query.k})")

        # 1. Recherche s√©mantique dans le vector store
        logger.info(f"Recherche de {query.k} documents contextuels...")
        results = vector_store.similarity_search_with_score(query.question, k=query.k)

        # 2. Formatage du contexte
        context_results = []
        context_parts = ["Voici les informations pertinentes trouv√©es dans la base de donn√©es:\n"]

        for i, (doc, score) in enumerate(results, 1):
            # Cr√©er le SearchResult pour la r√©ponse
            search_result = SearchResult(
                score=float(score),
                title=doc.metadata.get("title", "Sans titre"),
                content=doc.page_content,
                location=doc.metadata.get("location"),
                metadata=doc.metadata
            )
            context_results.append(search_result)

            # Formater pour le contexte textuel
            content_preview = doc.page_content[:500] + "..." if len(doc.page_content) > 500 else doc.page_content

            context_parts.append(f"\n--- R√©sultat {i} (pertinence: {score:.3f}) ---")
            context_parts.append(f"Titre: {doc.metadata.get('title', 'Sans titre')}")

            if doc.metadata.get("city"):
                context_parts.append(f"Ville: {doc.metadata['city']}")
            if doc.metadata.get("date_debut"):
                context_parts.append(f"Date d√©but: {doc.metadata['date_debut']}")
            if doc.metadata.get("date_fin"):
                context_parts.append(f"Date fin: {doc.metadata['date_fin']}")

            context_parts.append(f"\nContenu:\n{content_preview}")

        rag_context = "\n".join(context_parts)
        logger.info(f"‚úì {len(context_results)} documents trouv√©s pour le contexte")

        # 3. Construction du prompt enrichi
        enriched_prompt = f"""{rag_context}

---

Question de l'utilisateur:
{query.question}

R√©ponds √† la question en te basant sur les informations contextuelles ci-dessus. Si les informations ne permettent pas de r√©pondre compl√®tement, indique-le clairement."""

        # 4. Pr√©paration des messages pour Mistral AI
        # Utilise le prompt syst√®me personnalis√© si fourni, sinon utilise le prompt par d√©faut charg√© depuis ps.md
        system_prompt = query.system_prompt or default_system_prompt

        if not system_prompt:
            # Fallback en cas de probl√®me de chargement du fichier ps.md
            logger.warning("‚ö†Ô∏è  Aucun prompt syst√®me disponible, utilisation d'un prompt par d√©faut minimal")
            system_prompt = """Tu es un assistant sp√©cialis√© dans les √©v√©nements culturels de la r√©gion Occitanie.
Tu dois r√©pondre aux questions des utilisateurs en te basant UNIQUEMENT sur les informations fournies dans le contexte.
Si tu ne trouves pas l'information dans le contexte, dis-le clairement.
Sois pr√©cis, concis et utile."""

        messages = [
            SystemMessage(content=system_prompt, role="system"),
            UserMessage(content=enriched_prompt, role="user")
        ]

        # 5. Appel √† Mistral AI
        logger.info(f"Appel √† Mistral AI (mod√®le: {MISTRAL_MODEL})...")
        response = mistral_client.chat.complete(model=MISTRAL_MODEL, messages=messages)

        # 6. Extraction de la r√©ponse
        answer = response.choices[0].message.content

        # 7. Statistiques d'utilisation
        tokens_stats = {
            "prompt_tokens": response.usage.prompt_tokens,
            "completion_tokens": response.usage.completion_tokens,
            "total_tokens": response.usage.total_tokens
        }

        logger.info(f"‚úì R√©ponse g√©n√©r√©e (tokens: {tokens_stats['total_tokens']})")

        return AskResponse(
            question=query.question,
            answer=answer,
            context_used=context_results,
            tokens_used=tokens_stats
        )

    except Exception as e:
        logger.error(f"Erreur lors du traitement de la question: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=str(e))


async def run_rebuild_pipeline():
    """
    Ex√©cute le pipeline de mise √† jour incr√©mentale en arri√®re-plan.

    Cette fonction lance le script update_pipeline.py qui effectue:
    1. R√©cup√©ration de la date de derni√®re ex√©cution
    2. Backup et vidage des collections MongoDB
    3. R√©cup√©ration des agendas mis √† jour
    4. R√©cup√©ration des √©v√©nements
    5. D√©doublonnement
    6. Chunking et g√©n√©ration des embeddings
    7. Mise √† jour de l'index FAISS
    """
    global rebuild_in_progress

    try:
        rebuild_status["status"] = "running"
        rebuild_status["message"] = "Pipeline de mise √† jour en cours..."
        rebuild_status["started_at"] = datetime.now().isoformat()

        logger.info("=" * 70)
        logger.info("üîÑ D√âMARRAGE DU REBUILD DE L'INDEX FAISS")
        logger.info("=" * 70)

        # R√©cup√©rer la date de derni√®re mise √† jour
        from pymongo import MongoClient
        mongodb_uri = os.getenv("MONGODB_URI", "mongodb://localhost:27017/")
        db_name = os.getenv("MONGODB_DB_NAME", "OA")

        client = None
        last_update_date = None
        try:
            client = MongoClient(mongodb_uri)
            db = client[db_name]
            last_update_collection = db["last_update"]

            last_execution = last_update_collection.find_one(
                {}, sort=[("pipeline_run_date", -1)]
            )

            if last_execution and "pipeline_run_date" in last_execution:
                run_date = last_execution["pipeline_run_date"]
                if isinstance(run_date, datetime):
                    last_update_date = run_date.strftime("%Y-%m-%dT%H:%M:%S.000Z")
                else:
                    last_update_date = str(run_date)

                rebuild_status["last_update_date"] = last_update_date
                logger.info(f"‚úì Date de derni√®re ex√©cution: {last_update_date}")

            # V√©rifier s'il y a de nouveaux √©v√©nements depuis la derni√®re ex√©cution
            if last_update_date:
                events_collection = db[
                    os.getenv("MONGODB_COLLECTION_NAME_EVENTS", "events")
                ]

                # Compter les √©v√©nements cr√©√©s ou mis √† jour depuis la derni√®re ex√©cution
                new_events_count = events_collection.count_documents({
                    "$or": [
                        {"createdAt": {"$gte": last_update_date}},
                        {"updatedAt": {"$gte": last_update_date}}
                    ]
                })

                logger.info(
                    f"üìä √âv√©nements nouveaux/modifi√©s depuis la derni√®re "
                    f"ex√©cution: {new_events_count}"
                )

                if new_events_count == 0:
                    logger.warning("‚ö†Ô∏è  Aucun nouvel √©v√©nement d√©tect√©")
                    rebuild_status["status"] = "warning"
                    rebuild_status["message"] = (
                        "Pas de nouveaux √©v√©nements depuis la derni√®re ex√©cution. "
                        "Rebuild annul√©."
                    )
                    rebuild_in_progress = False
                    return

        finally:
            if client:
                client.close()

        # Construire le chemin vers le script update_pipeline.py
        project_root = os.path.dirname(os.path.dirname(os.path.dirname(__file__)))
        script_path = os.path.join(project_root, "src", "update_pipeline.py")

        # Ex√©cuter le pipeline de mise √† jour
        logger.info(f"Ex√©cution du script: {script_path}")
        process = await asyncio.create_subprocess_exec(
            "uv", "run", "python", script_path,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE,
            cwd=project_root
        )

        stdout, stderr = await process.communicate()

        if process.returncode == 0:
            logger.info("‚úÖ Pipeline de mise √† jour termin√© avec succ√®s")
            logger.info("üîÑ Rechargement de l'index FAISS en m√©moire...")

            # Recharger le vector store avec le nouvel index
            try:
                global vector_store
                vector_store = load_vector_store(
                    load_path=FAISS_INDEX_PATH,
                    embeddings=embeddings_model,
                    verbose=False
                )

                # Afficher les nouvelles statistiques
                stats = get_vector_store_stats(vector_store)
                logger.info("‚úÖ Nouvel index FAISS charg√© en m√©moire")
                logger.info(f"  - Nombre de vecteurs: {stats['num_vectors']:,}")
                logger.info(f"  - Dimension: {stats['dimension']}")

                rebuild_status["status"] = "success"
                rebuild_status["message"] = (
                    "Pipeline termin√© avec succ√®s. "
                    "Nouvel index FAISS charg√© automatiquement."
                )
            except Exception as reload_error:
                logger.error(
                    f"‚ùå Erreur lors du rechargement de l'index: {reload_error}",
                    exc_info=True
                )
                rebuild_status["status"] = "success_with_warning"
                rebuild_status["message"] = (
                    "Pipeline termin√© avec succ√®s mais √©chec du rechargement. "
                    "Red√©marrez l'API manuellement pour charger le nouvel index."
                )

        else:
            error_msg = stderr.decode() if stderr else "Erreur inconnue"
            rebuild_status["status"] = "error"
            rebuild_status["message"] = f"√âchec du pipeline: {error_msg}"
            logger.error(f"‚ùå √âchec du pipeline de mise √† jour: {error_msg}")

    except Exception as e:
        rebuild_status["status"] = "error"
        rebuild_status["message"] = f"Erreur lors du rebuild: {str(e)}"
        logger.error(f"‚ùå Erreur lors du rebuild: {e}", exc_info=True)
    finally:
        rebuild_in_progress = False


@app.post("/rebuild", response_model=RebuildResponse)
async def rebuild_index(background_tasks: BackgroundTasks):
    """
    Lance le pipeline de mise √† jour incr√©mentale de l'index FAISS.

    Cette endpoint d√©clenche le pipeline de mise √† jour qui:
    1. R√©cup√®re la date de derni√®re ex√©cution
    2. Sauvegarde et vide les collections MongoDB
    3. R√©cup√®re les agendas mis √† jour depuis la derni√®re ex√©cution
    4. R√©cup√®re les √©v√©nements pour ces agendas (avec filtre de date)
    5. D√©doublonne les √©v√©nements
    6. G√©n√®re les chunks et les embeddings
    7. Reconstruit l'index FAISS

    Le pipeline s'ex√©cute en arri√®re-plan. Utilisez GET /rebuild/status
    pour suivre la progression.

    IMPORTANT: Une fois le rebuild termin√©, vous devez red√©marrer l'API
    pour charger le nouvel index FAISS en m√©moire.

    Returns:
        RebuildResponse avec le statut de l'op√©ration
    """
    global rebuild_in_progress

    if rebuild_in_progress:
        return RebuildResponse(
            status="running",
            message="Un rebuild est d√©j√† en cours",
            last_update_date=rebuild_status.get("last_update_date"),
            details={
                "started_at": rebuild_status.get("started_at"),
                "current_status": rebuild_status.get("message")
            }
        )

    rebuild_in_progress = True
    background_tasks.add_task(run_rebuild_pipeline)

    return RebuildResponse(
        status="started",
        message=(
            "Pipeline de mise √† jour d√©marr√© en arri√®re-plan. "
            "Utilisez GET /rebuild/status pour suivre la progression."
        ),
        last_update_date=None,
        details={"started_at": datetime.now().isoformat()}
    )


@app.get("/rebuild/status", response_model=RebuildResponse)
async def rebuild_status_endpoint():
    """
    Retourne le statut du rebuild en cours ou du dernier rebuild.

    Returns:
        RebuildResponse avec le statut actuel
    """
    return RebuildResponse(
        status=rebuild_status["status"],
        message=rebuild_status["message"],
        last_update_date=rebuild_status.get("last_update_date"),
        details={
            "started_at": rebuild_status.get("started_at"),
            "in_progress": rebuild_in_progress
        }
    )


if __name__ == "__main__":
    import uvicorn

    # D√©marrage du serveur en mode d√©veloppement
    uvicorn.run(
        "main:app",
        host="0.0.0.0",
        port=8000,
        reload=True,
        log_level="info"
    )
