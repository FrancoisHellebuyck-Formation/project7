"""
Pipeline complet pour la cr√©ation de la base vectorielle depuis MongoDB.

Ce module orchestre l'ensemble du processus :
1. Connexion √† MongoDB
2. Chargement et chunking des documents
   2.5. Suppression de l'index FAISS existant (apr√®s validation des donn√©es)
3. G√©n√©ration des embeddings et cr√©ation de l'index FAISS
4. Sauvegarde et test de recherche
"""

from typing import Optional, Dict, Any
import os
import logging
from datetime import datetime, timezone, timedelta
from dotenv import load_dotenv

from langchain_community.vectorstores import FAISS
from pymongo import MongoClient

from embeddings import get_embeddings_model
from vectors import (
    create_vector_store,
    save_vector_store,
    search_similar_documents,
    delete_vector_store,
)
from chunks.chunks_document import get_mongodb_connection, process_events_to_chunks

# Configuration du logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def get_default_updated_date() -> str:
    """
    Calcule la date par d√©faut pour updatedAt.gte (aujourd'hui - 1 an).

    Returns:
        str: Date au format ISO 8601 (ex: "2024-11-03T00:00:00.000Z")
    """
    one_year_ago = datetime.now(timezone.utc) - timedelta(days=365)
    return one_year_ago.strftime("%Y-%m-%dT%H:%M:%S.000Z")


def calculate_months_back(date_str: str) -> int:
    """
    Calcule le nombre de mois entre une date donn√©e et aujourd'hui.

    Args:
        date_str: Date au format ISO 8601 (ex: "2024-01-01T00:00:00.000Z")

    Returns:
        int: Nombre de mois approximatif (bas√© sur 30 jours par mois)
    """
    try:
        # Parser la date ISO 8601
        if date_str.endswith('Z'):
            date_str = date_str[:-1] + '+00:00'

        # Conversion en datetime
        target_date = datetime.fromisoformat(date_str)
        now = datetime.now(timezone.utc)

        # Calculer la diff√©rence en jours
        days_diff = (now - target_date).days

        # Convertir en mois (approximation: 30 jours par mois)
        months = round(days_diff / 30)

        return max(0, months)  # Ne pas retourner de valeur n√©gative
    except Exception as e:
        logger.warning(f"Erreur lors du calcul des mois: {e}")
        return 12  # Valeur par d√©faut: 12 mois


def get_last_execution_date(verbose: bool = False) -> str:
    """
    R√©cup√®re la date de la derni√®re ex√©cution du pipeline depuis MongoDB.

    Args:
        verbose: Si True, affiche des informations

    Returns:
        str: Date de la derni√®re ex√©cution au format ISO 8601, ou None si aucune ex√©cution
    """
    load_dotenv()

    mongodb_uri = os.getenv("MONGODB_URI", "mongodb://localhost:27017/")
    db_name = os.getenv("MONGODB_DB_NAME", "OA")

    client = None
    try:
        client = MongoClient(mongodb_uri)
        db = client[db_name]
        last_update_collection = db["last_update"]

        # R√©cup√©rer la derni√®re ex√©cution
        last_execution = last_update_collection.find_one(
            {},
            sort=[("pipeline_run_date", -1)]
        )

        if last_execution and "pipeline_run_date" in last_execution:
            run_date = last_execution["pipeline_run_date"]
            # Convertir en format ISO 8601
            if isinstance(run_date, datetime):
                iso_date = run_date.strftime("%Y-%m-%dT%H:%M:%S.000Z")
            else:
                iso_date = str(run_date)

            if verbose:
                logger.info(f"Derni√®re ex√©cution trouv√©e: {iso_date}")
            return iso_date
        else:
            if verbose:
                logger.info("Aucune ex√©cution pr√©c√©dente trouv√©e")
            return None

    except Exception as e:
        logger.warning(f"Erreur lors de la r√©cup√©ration de la derni√®re ex√©cution: {e}")
        return None
    finally:
        if client:
            client.close()


def save_last_update_metadata(
    updated_at_gte: str,
    months_back: int,
    total_chunks: int,
    total_events: int,
    verbose: bool = False
) -> None:
    """
    Sauvegarde les m√©tadonn√©es de la derni√®re mise √† jour dans MongoDB.

    Args:
        updated_at_gte: Date de s√©lection utilis√©e pour filtrer les agendas
        months_back: Nombre de mois en arri√®re recherch√©s
        total_chunks: Nombre total de chunks cr√©√©s
        total_events: Nombre total d'√©v√©nements trait√©s
        verbose: Si True, affiche des informations de progression
    """
    load_dotenv()

    mongodb_uri = os.getenv("MONGODB_URI", "mongodb://localhost:27017/")
    db_name = os.getenv("MONGODB_DB_NAME", "OA")

    client = None
    try:
        client = MongoClient(mongodb_uri)
        db = client[db_name]
        last_update_collection = db["last_update"]

        # Pr√©parer le document de m√©tadonn√©es
        metadata = {
            "pipeline_run_date": datetime.now(timezone.utc),
            "agendas_updated_at_gte": updated_at_gte,
            "months_back": months_back,
            "total_events_processed": total_events,
            "total_chunks_created": total_chunks,
            "region": os.getenv("OA_REGION", "N/A"),
            "embeddings_model": os.getenv("EMBEDDINGS_MODEL", "intfloat/multilingual-e5-large"),
            "chunk_size": int(os.getenv("CHUNK_SIZE", "500")),
            "chunk_overlap": int(os.getenv("CHUNK_OVERLAP", "100")),
        }

        if verbose:
            logger.info("\n" + "=" * 70)
            logger.info("üíæ SAUVEGARDE DES M√âTADONN√âES")
            logger.info("=" * 70)
            logger.info(f"Date de s√©lection: {updated_at_gte}")
            logger.info(f"Mois recherch√©s: {months_back}")
            logger.info(f"√âv√©nements trait√©s: {total_events}")
            logger.info(f"Chunks cr√©√©s: {total_chunks}")

        # Ins√©rer le document (on garde l'historique)
        last_update_collection.insert_one(metadata)

        if verbose:
            logger.info("‚úÖ M√©tadonn√©es sauvegard√©es dans la collection 'last_update'")
            logger.info("=" * 70)

    except Exception as e:
        logger.error(f"‚ùå Erreur lors de la sauvegarde des m√©tadonn√©es: {e}", exc_info=True)
    finally:
        if client:
            client.close()


def create_vector_store_pipeline(
    save_path: Optional[str] = None,
    mongodb_query: Optional[Dict[str, Any]] = None,
    limit: Optional[int] = None,
    chunk_size: int = 400,
    chunk_overlap: int = 100,
    model_id: Optional[str] = None,
    device: Optional[str] = None,
    batch_size: int = 32,
    verbose: bool = False,
) -> FAISS:
    """
    Pipeline complet: MongoDB ‚Üí chunks ‚Üí embeddings ‚Üí FAISS.

    Args:
        save_path: Chemin pour sauvegarder le vector store (optionnel)
        mongodb_query: Filtre MongoDB pour s√©lectionner les √©v√©nements
        limit: Nombre maximum d'√©v√©nements √† traiter
        chunk_size: Taille des chunks en caract√®res
        chunk_overlap: Chevauchement entre chunks
        model_id: Identifiant du mod√®le d'embeddings
        device: Device √† utiliser ('cuda', 'mps', 'cpu')
        batch_size: Taille des batchs pour les embeddings
        verbose: Si True, affiche des informations de progression

    Returns:
        FAISS: Instance du vector store cr√©√©

    Raises:
        ValueError: Si aucun chunk n'a pu √™tre cr√©√©
    """
    if verbose:
        logger.info("=" * 70)
        logger.info("PIPELINE DE CR√âATION DU VECTOR STORE")
        logger.info("=" * 70)

    # 1. Connexion √† MongoDB
    if verbose:
        logger.info("\n[1/4] Connexion √† MongoDB...")
    client, events_collection = get_mongodb_connection()

    try:
        # 2. Chargement et chunking des documents
        if verbose:
            logger.info("\n[2/4] Chargement et d√©coupage des documents...")
        chunks = process_events_to_chunks(
            events_collection=events_collection,
            query=mongodb_query,
            limit=limit,
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            verbose=verbose,
        )

        if not chunks:
            raise ValueError(
                "Aucun chunk cr√©√©. V√©rifiez que des √©v√©nements existent dans MongoDB."
            )

        # 2.5. Suppression de l'index FAISS existant (apr√®s v√©rification des donn√©es)
        if save_path:
            if verbose:
                logger.info("\n[2.5/4] Suppression de l'index FAISS existant...")
            delete_vector_store(save_path, verbose=verbose)

        # 3. Cr√©ation des embeddings et du vector store
        if verbose:
            logger.info(
                "\n[3/4] G√©n√©ration des embeddings et cr√©ation du vector store..."
            )
            logger.info(f"      Nombre de chunks: {len(chunks)}")
            logger.info(f"      Mod√®le: {model_id or 'intfloat/multilingual-e5-large'}")
            logger.info(f"      Device: {device or 'auto-d√©tect√©'}")
            logger.info(f"      Batch size: {batch_size}")
            logger.info("      Cette √©tape peut prendre plusieurs minutes...")

        embeddings = get_embeddings_model(
            model_id=model_id, device=device, batch_size=batch_size
        )
        vector_store = create_vector_store(chunks, embeddings, verbose=verbose)

        # 4. Sauvegarde du vector store
        if save_path:
            if verbose:
                logger.info("\n[4/4] Sauvegarde du vector store...")
            save_vector_store(vector_store, save_path, verbose=verbose)
        else:
            if verbose:
                logger.info("\n[4/4] Sauvegarde ignor√©e (aucun chemin sp√©cifi√©)")

        if verbose:
            logger.info("\n" + "=" * 70)
            logger.info("‚úì PIPELINE TERMIN√â AVEC SUCC√àS")
            logger.info("=" * 70)

        return vector_store, len(chunks)

    finally:
        client.close()
        if verbose:
            logger.info("Connexion MongoDB ferm√©e")


def main():
    """
    Fonction principale pour ex√©cuter le pipeline complet.
    Configuration via variables d'environnement et arguments de ligne de commande.

    Arguments:
        mode: 'update' ou 'recreate' (d√©faut: 'recreate')
            - update: Mode incr√©mental, traite uniquement les nouveaux √©v√©nements depuis la derni√®re ex√©cution
            - recreate: Mode complet, recr√©e tout l'index depuis le d√©but
    """
    import sys

    # Chargement des variables d'environnement
    load_dotenv()

    # D√©terminer le mode (update ou recreate)
    mode = sys.argv[1] if len(sys.argv) > 1 else "recreate"

    if mode not in ["update", "recreate"]:
        logger.error(f"Mode invalide: {mode}. Utilisez 'update' ou 'recreate'")
        sys.exit(1)

    # Configuration
    save_path = os.getenv("FAISS_INDEX_PATH", "data/faiss_index")
    limit = os.getenv("EMBEDDINGS_LIMIT")
    limit = int(limit) if limit else None

    model_id = os.getenv("EMBEDDINGS_MODEL", "intfloat/multilingual-e5-large")
    device = os.getenv("EMBEDDINGS_DEVICE") or None  # None = auto-d√©tection
    batch_size = int(os.getenv("EMBEDDINGS_BATCH_SIZE", "32"))

    try:
        logger.info("=" * 70)
        logger.info(f"MODE: {mode.upper()}")
        logger.info("=" * 70)

        # D√©terminer la date de filtrage selon le mode
        if mode == "update":
            # Mode incr√©mental: utiliser la date de la derni√®re ex√©cution
            logger.info("Mode UPDATE: Recherche de la derni√®re ex√©cution...")
            last_execution_date = get_last_execution_date(verbose=True)

            if last_execution_date:
                # Utiliser cette date comme date minimale de mise √† jour
                os.environ["OA_AGENDAS_UPDATED_AT_GTE"] = last_execution_date
                logger.info(f"‚úì Date de mise √† jour minimale: {last_execution_date}")
                logger.info("  Seuls les √©v√©nements nouveaux/modifi√©s seront trait√©s")
            else:
                logger.warning("‚ö†Ô∏è  Aucune ex√©cution pr√©c√©dente trouv√©e")
                logger.info("   Passage en mode RECREATE (traitement complet)")
                mode = "recreate"

        if mode == "recreate":
            # Mode complet: utiliser la date par d√©faut ou celle du .env
            logger.info("Mode RECREATE: Reconstruction compl√®te de l'index")
            updated_at_gte = os.getenv("OA_AGENDAS_UPDATED_AT_GTE")
            if updated_at_gte:
                logger.info(f"‚úì Date de mise √† jour minimale: {updated_at_gte} (depuis .env)")
            else:
                default_date = get_default_updated_date()
                logger.info(f"‚úì Date de mise √† jour minimale: {default_date} (par d√©faut: 1 an)")

        logger.info("=" * 70)
        logger.info("D√©marrage du pipeline de cr√©ation du vector store...")

        # Ex√©cution du pipeline complet
        vector_store, total_chunks = create_vector_store_pipeline(
            save_path=save_path,
            limit=limit,
            model_id=model_id,
            device=device,
            batch_size=batch_size,
            verbose=True,
        )

        # Sauvegarde des m√©tadonn√©es de mise √† jour
        # Utiliser la valeur de .env ou calculer la date par d√©faut (1 an en arri√®re)
        updated_at_gte = os.getenv("OA_AGENDAS_UPDATED_AT_GTE")
        if not updated_at_gte:
            updated_at_gte = get_default_updated_date()
            logger.info(f"Variable OA_AGENDAS_UPDATED_AT_GTE non d√©finie, utilisation de la date par d√©faut: {updated_at_gte}")

        months_back = calculate_months_back(updated_at_gte)

        # Compter le nombre d'√©v√©nements dans MongoDB
        client_meta = MongoClient(os.getenv("MONGODB_URI", "mongodb://localhost:27017/"))
        db_meta = client_meta[os.getenv("MONGODB_DB_NAME", "OA")]
        events_collection_meta = db_meta[os.getenv("MONGODB_COLLECTION_NAME_EVENTS", "events")]
        total_events = events_collection_meta.count_documents({})
        client_meta.close()

        save_last_update_metadata(
            updated_at_gte=updated_at_gte,
            months_back=months_back,
            total_chunks=total_chunks,
            total_events=total_events,
            verbose=True
        )

        # Test de recherche (optionnel)
        test_query = os.getenv("TEST_QUERY")
        if test_query:
            logger.info(f"\n{'='*70}")
            logger.info("üîç TEST DE RECHERCHE S√âMANTIQUE")
            logger.info(f"{'='*70}")
            logger.info(f"Requ√™te: '{test_query}'")

            results = search_similar_documents(
                vector_store, test_query, k=5, verbose=True
            )

            logger.info(f"\n{'='*70}")
            logger.info("üìä R√âSULTATS D√âTAILL√âS")
            logger.info("=" * 70)

            for i, (doc, score) in enumerate(results, 1):
                logger.info(f"\n--- R√©sultat {i} (Score: {score:.4f}) ---")
                logger.info(f"Titre: {doc.metadata.get('title', 'N/A')}")
                logger.info(f"Lieu: {doc.metadata.get('locationName', 'N/A')}")
                logger.info(f"Date: {doc.metadata.get('dateRange', 'N/A')}")
                logger.info(f"R√©gion: {doc.metadata.get('region', 'N/A')}")
                logger.info(f"\nExtrait: {doc.page_content[:300]}...")
                logger.info("-" * 70)

        logger.info("\n‚úì Programme termin√© avec succ√®s")

    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'ex√©cution du pipeline: {e}", exc_info=True)
        raise


if __name__ == "__main__":
    main()
