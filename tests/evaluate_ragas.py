"""
Script d'√©valuation RAGAS pour le syst√®me RAG.

Ce script √©value la qualit√© du RAG (Retrieval Augmented Generation) en utilisant
le framework RAGAS. Il g√©n√®re un rapport avec les m√©triques suivantes :
- Faithfulness : Fid√©lit√© de la r√©ponse au contexte
- Answer Relevancy : Pertinence de la r√©ponse √† la question
- Context Precision : Pr√©cision du contexte r√©cup√©r√©
- Context Recall : Compl√©tude du contexte r√©cup√©r√©

Pr√©requis :
- API FastAPI d√©marr√©e (make run-api)
- Index FAISS cr√©√© (make run-embeddings)
- Variables d'environnement configur√©es (.env et .env.test)

Usage:
    python tests/evaluate_ragas.py [fichier_questions.json]
    make test-ragas

Arguments:
    fichier_questions.json : Optionnel, chemin vers le fichier de questions
                             (d√©faut: ragas_test_questions_collected.json)
"""

import os
import sys
import time
import json
import argparse
import requests
from typing import List, Dict, Any
from pathlib import Path
from dotenv import load_dotenv
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
)
from langchain_mistralai import ChatMistralAI
from langchain_huggingface import HuggingFaceEmbeddings


# ============================================================================
# Configuration
# ============================================================================

# Charger les variables d'environnement
env_path = Path(__file__).parent.parent / ".env"
test_env_path = Path(__file__).parent.parent / ".env.test"

if env_path.exists():
    load_dotenv(env_path)
if test_env_path.exists():
    load_dotenv(test_env_path, override=True)

# Configuration
API_URL = os.getenv("RAG_API_URL", "http://localhost:8000")
RAGAS_TOP_K = int(os.getenv("RAGAS_TOP_K", "5"))
RAGAS_API_TIMEOUT = float(os.getenv("RAGAS_API_TIMEOUT", "30"))
RAGAS_MISTRAL_DELAY = float(os.getenv("RAGAS_MISTRAL_DELAY", "2.0"))
MISTRAL_API_KEY = os.getenv("MISTRAL_API_KEY")
MISTRAL_MODEL = os.getenv("MISTRAL_MODEL", "mistral-small-latest")
EMBEDDINGS_MODEL = os.getenv("EMBEDDINGS_MODEL", "intfloat/multilingual-e5-large")


# ============================================================================
# Dataset de test
# ============================================================================

def load_test_questions(json_path: str = None) -> List[Dict[str, Any]]:
    """
    Charge les cas de test depuis un fichier JSON.

    Par d√©faut, utilise UNIQUEMENT le fichier ragas_test_questions_collected.json.
    Si ce fichier n'existe pas, le script s'arr√™te avec une erreur explicite.

    Pour g√©n√©rer ce fichier: make collect-ragas

    Structure attendue du JSON:
    {
      "test_cases": [
        {
          "id": "test_001",
          "question": "...",
          "answer": "..." ou null,
          "contexts": [...] ou null,
          "ground_truth": "...",
          "category": "...",
          "location": "...",
          "notes": "..."
        }
      ]
    }

    Args:
        json_path : Chemin vers le fichier JSON (optionnel, utilise ragas_test_questions_collected.json si None)

    Returns:
        list : Liste de dictionnaires avec question, answer, contexts, ground_truth
    """
    if json_path is None:
        # Utiliser UNIQUEMENT le fichier collected
        collected_path = Path(__file__).parent / "ragas_test_questions_collected.json"

        if not collected_path.exists():
            print("\n‚ùå ERREUR: Fichier de donn√©es collect√©es introuvable")
            print(f"   Fichier attendu: {collected_path}")
            print("")
            print("üìã Pour g√©n√©rer ce fichier:")
            print("   1. D√©marrez l'API: make run-api")
            print("   2. Collectez les donn√©es: make collect-ragas")
            print("   3. Relancez l'√©valuation: make test-ragas")
            print("")
            sys.exit(1)

        json_path = collected_path
        print(f"üì¶ Utilisation du fichier pr√©-collect√©: {collected_path.name}")
    else:
        json_path = Path(json_path)

    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # Nouvelle structure: "test_cases" au lieu de "questions"
        test_cases = data.get("test_cases", [])
        if not test_cases:
            print(f"‚ö†Ô∏è  Aucun cas de test trouv√© dans {json_path}")
            return []

        print(f"üìã Charg√© {len(test_cases)} cas de test depuis {json_path.name}")
        if "description" in data:
            print(f"   Description: {data['description']}")
        if "version" in data:
            print(f"   Version: {data['version']}")

        # Compter combien ont d√©j√† answer/contexts pr√©-collect√©s
        pre_collected = sum(1 for tc in test_cases if tc.get("answer") and tc.get("contexts"))
        if pre_collected > 0:
            print(f"   ‚úì {pre_collected}/{len(test_cases)} cas avec answer/contexts pr√©-collect√©s")
        else:
            print("   ‚ö†Ô∏è  Aucun cas pr√©-collect√©. Les r√©ponses seront collect√©es dynamiquement.")

        return test_cases

    except FileNotFoundError:
        print(f"‚ùå Fichier non trouv√©: {json_path}")
        print("   Utilisation des cas de test par d√©faut")
        # Cas de test de fallback
        return [
            {
                "id": "fallback_001",
                "question": "Quels sont les √©v√©nements culturels gratuits √† Toulouse ?",
                "answer": None,
                "contexts": None,
                "ground_truth": "Il existe plusieurs √©v√©nements culturels gratuits √† Toulouse comme les concerts dans les parcs, les expositions municipales et les festivals de rue.",
            },
            {
                "id": "fallback_002",
                "question": "O√π puis-je trouver des expositions d'art contemporain en Occitanie ?",
                "answer": None,
                "contexts": None,
                "ground_truth": "Les expositions d'art contemporain en Occitanie sont disponibles dans plusieurs mus√©es et galeries √† Toulouse, Montpellier et dans d'autres villes de la r√©gion.",
            },
            {
                "id": "fallback_003",
                "question": "Quels festivals de musique ont lieu en √©t√© en Occitanie ?",
                "answer": None,
                "contexts": None,
                "ground_truth": "Plusieurs festivals de musique ont lieu en √©t√© en Occitanie, incluant des festivals de jazz, de musique classique et de musiques du monde.",
            },
        ]
    except json.JSONDecodeError as e:
        print(f"‚ùå Erreur lors du parsing JSON: {e}")
        print("   V√©rifiez la syntaxe du fichier JSON")
        sys.exit(1)
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement des cas de test: {e}")
        sys.exit(1)


# ============================================================================
# Fonctions utilitaires
# ============================================================================

def check_api_health() -> bool:
    """
    V√©rifie que l'API RAG est accessible et fonctionnelle.

    Returns:
        bool : True si l'API est accessible, False sinon
    """
    try:
        response = requests.get(f"{API_URL}/health", timeout=5)
        if response.status_code == 200:
            data = response.json()
            status_ok = data.get("status") in ["ok", "healthy"]
            vector_store_ok = data.get("vector_store_loaded", False)
            embeddings_ok = data.get("embeddings_model_loaded", False)
            return status_ok and vector_store_ok and embeddings_ok
        return False
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Erreur lors du health check: {e}")
        return False


def get_rag_response(question: str) -> Dict[str, Any]:
    """
    R√©cup√®re une r√©ponse du syst√®me RAG via l'API.

    Args:
        question : Question √† poser

    Returns:
        dict : R√©ponse contenant answer, context_used, tokens_used

    Raises:
        requests.exceptions.HTTPError : Si l'API retourne une erreur
    """
    try:
        response = requests.post(
            f"{API_URL}/ask",
            json={"question": question, "k": RAGAS_TOP_K},
            timeout=RAGAS_API_TIMEOUT
        )
        response.raise_for_status()
        result = response.json()

        # Appliquer le d√©lai apr√®s un appel r√©ussi
        if RAGAS_MISTRAL_DELAY > 0:
            time.sleep(RAGAS_MISTRAL_DELAY)

        return result
    except requests.exceptions.HTTPError as e:
        # Gestion sp√©ciale pour les erreurs 429 (rate limit Mistral)
        if e.response.status_code == 500:
            try:
                error_detail = e.response.json().get("detail", "")
                if "429" in error_detail or "capacity exceeded" in error_detail.lower():
                    print(f"\n‚ö†Ô∏è  API Mistral a d√©pass√© son quota (429) pour la question: {question}")
                    print("   R√©essayez plus tard ou augmentez votre tier.")
                    return None
            except Exception:
                pass
        print(f"‚ùå Erreur HTTP lors de la requ√™te RAG: {e}")
        raise
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Erreur lors de la requ√™te RAG: {e}")
        raise


def format_contexts_for_ragas(context_used: List[Dict[str, Any]]) -> List[str]:
    """
    Formate les contextes r√©cup√©r√©s pour l'√©valuation RAGAS.

    Utilise le contenu de l'√©v√©nement pour l'√©valuation RAGAS.
    Cela permet √† RAGAS d'√©valuer la pertinence bas√©e sur le contenu
    textuel complet des √©v√©nements r√©cup√©r√©s.

    Args:
        context_used : Liste des contextes utilis√©s par le RAG

    Returns:
        list : Liste de contenus d'√©v√©nements (strings)
    """
    return [ctx.get("content", "") for ctx in context_used]


def collect_rag_data(test_cases: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Collecte les donn√©es du syst√®me RAG pour tous les cas de test.

    Si un cas de test a d√©j√† answer et contexts (pr√©-collect√©s), ils sont utilis√©s.
    Sinon, le script interroge l'API RAG pour collecter ces donn√©es dynamiquement.

    Args:
        test_cases : Liste des cas de test charg√©s depuis JSON

    Returns:
        list : Liste de dictionnaires avec question, answer, contexts, ground_truth
    """
    print("\n" + "=" * 70)
    print("üìä COLLECTE DES DONN√âES POUR L'√âVALUATION RAGAS")
    print("=" * 70)
    print(f"\nNombre de cas de test: {len(test_cases)}")
    print(f"Configuration: top_k={RAGAS_TOP_K}, timeout={RAGAS_API_TIMEOUT}s, delay={RAGAS_MISTRAL_DELAY}s")
    print("")

    results = []
    collected_count = 0
    pre_collected_count = 0

    for i, test_case in enumerate(test_cases, 1):
        question = test_case["question"]
        ground_truth = test_case["ground_truth"]
        test_id = test_case.get("id", f"test_{i:03d}")

        print(f"\n[{i}/{len(test_cases)}] {test_id}")
        print(f"   Question: {question}")

        # V√©rifier si answer et contexts sont d√©j√† fournis
        if test_case.get("answer") and test_case.get("contexts"):
            print("   ‚úì Donn√©es pr√©-collect√©es trouv√©es dans le JSON")
            answer = test_case["answer"]
            contexts = test_case["contexts"]

            # V√©rifier la validit√© des donn√©es pr√©-collect√©es
            if not answer or len(answer) < 50:
                print(f"   ‚ö†Ô∏è  R√©ponse pr√©-collect√©e trop courte ({len(answer)} caract√®res), collecte dynamique...")
            elif not contexts or len(contexts) == 0:
                print("   ‚ö†Ô∏è  Aucun contexte pr√©-collect√©, collecte dynamique...")
            else:
                # Utiliser les donn√©es pr√©-collect√©es
                # contexts peut √™tre une liste de strings ou une liste de dicts
                if isinstance(contexts[0], dict):
                    # Format API: list of dicts with 'content' key
                    formatted_contexts = format_contexts_for_ragas(contexts)
                else:
                    # Format RAGAS: list of strings (d√©j√† format√©)
                    formatted_contexts = contexts

                results.append({
                    "question": question,
                    "answer": answer,
                    "contexts": formatted_contexts,
                    "ground_truth": ground_truth
                })

                print(f"   ‚úÖ Pr√©-collect√©: {len(answer)} caract√®res, {len(formatted_contexts)} contextes")
                pre_collected_count += 1
                continue

        # Pas de donn√©es pr√©-collect√©es ou invalides -> collecte dynamique
        print("   üîç Collecte dynamique via API RAG...")

        # R√©cup√©rer la r√©ponse RAG
        response = get_rag_response(question)

        if response is None:
            print("   ‚ö†Ô∏è  Cas ignor√© (erreur 429 ou timeout)")
            continue

        # V√©rifier la structure de la r√©ponse
        if "answer" not in response or "context_used" not in response:
            print("   ‚ùå R√©ponse invalide (structure incorrecte)")
            continue

        answer = response["answer"]
        contexts = response["context_used"]

        # V√©rifier que la r√©ponse et les contextes sont valides
        if not answer or len(answer) < 50:
            print(f"   ‚ö†Ô∏è  R√©ponse trop courte ({len(answer)} caract√®res), ignor√©e")
            continue

        if len(contexts) == 0:
            print("   ‚ö†Ô∏è  Aucun contexte r√©cup√©r√©, cas ignor√©")
            continue

        # Collecter les donn√©es
        results.append({
            "question": question,
            "answer": answer,
            "contexts": format_contexts_for_ragas(contexts),
            "ground_truth": ground_truth
        })

        print(f"   ‚úÖ Collect√©: {len(answer)} caract√®res, {len(contexts)} contextes")
        collected_count += 1

    print(f"\n{'=' * 70}")
    print(f"‚úÖ Collecte termin√©e: {len(results)}/{len(test_cases)} cas trait√©s")
    if pre_collected_count > 0:
        print(f"   - {pre_collected_count} cas pr√©-collect√©s")
    if collected_count > 0:
        print(f"   - {collected_count} cas collect√©s dynamiquement")
    print("=" * 70)

    return results


def generate_ragas_report(ragas_data: List[Dict[str, Any]]):
    """
    G√©n√®re le rapport final RAGAS avec les m√©triques calcul√©es.

    Args:
        ragas_data : Liste de dictionnaires avec question, answer, contexts, ground_truth
    """
    if not ragas_data:
        print("\n‚ö†Ô∏è  Aucune donn√©e √† √©valuer. Toutes les questions ont √©t√© ignor√©es.")
        print("   Causes possibles:")
        print("   - API Mistral a d√©pass√© son quota (429)")
        print("   - R√©ponses trop courtes ou contextes manquants")
        print("   - Probl√®me de connexion √† l'API")
        return

    print("\n" + "=" * 70)
    print("üéØ G√âN√âRATION DU RAPPORT RAGAS")
    print("=" * 70)

    try:
        # Cr√©er le dataset pour RAGAS
        dataset_dict = {
            "question": [r["question"] for r in ragas_data],
            "answer": [r["answer"] for r in ragas_data],
            "contexts": [r["contexts"] for r in ragas_data],
            "ground_truth": [r["ground_truth"] for r in ragas_data],
        }

        dataset = Dataset.from_dict(dataset_dict)

        print(f"\nüìä √âvaluation de {len(ragas_data)} questions...")
        print("‚è≥ Calcul des m√©triques RAGAS en cours...")
        print("   (Cette op√©ration peut prendre 30-60 secondes)")
        print("")

        # V√©rifier la cl√© API
        if not MISTRAL_API_KEY:
            print("‚ùå MISTRAL_API_KEY non configur√©e. Impossible d'√©valuer avec RAGAS.")
            return

        # Configurer le LLM Mistral AI pour RAGAS
        print(f"‚öôÔ∏è  Configuration LLM: model={MISTRAL_MODEL}, timeout={RAGAS_API_TIMEOUT}s, delay={RAGAS_MISTRAL_DELAY}s")
        print("   Note: RAGAS effectue plusieurs appels API pour calculer les m√©triques")
        print("   En cas d'erreur 429, augmentez RAGAS_MISTRAL_DELAY dans .env.test")
        print("")

        llm = ChatMistralAI(
            model=MISTRAL_MODEL,
            api_key=MISTRAL_API_KEY,
            temperature=0.0,
            max_retries=3,
            timeout=RAGAS_API_TIMEOUT
        )

        # Configurer les embeddings pour RAGAS (utilise E5 au lieu d'OpenAI)
        print(f"üì¶ Configuration embeddings: {EMBEDDINGS_MODEL}")
        embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDINGS_MODEL,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )

        # √âvaluer avec les m√©triques RAGAS
        metrics = [faithfulness, answer_relevancy, context_precision, context_recall]

        # D√©lai avant l'√©valuation pour r√©cup√©ration du quota
        if RAGAS_MISTRAL_DELAY > 0:
            print(f"‚è≥ Attente de {RAGAS_MISTRAL_DELAY}s avant d'√©valuer (r√©cup√©ration du quota API)...")
            time.sleep(RAGAS_MISTRAL_DELAY)

        result = evaluate(dataset, metrics=metrics, llm=llm, embeddings=embeddings)

        # Convertir en DataFrame si n√©cessaire
        df = None
        if hasattr(result, 'to_pandas'):
            df = result.to_pandas()

        # Afficher les scores d√©taill√©s par question
        if df is not None and len(df) > 0:
            print("\n" + "=" * 70)
            print("üìä SCORES D√âTAILL√âS PAR QUESTION")
            print("=" * 70)
            print("")

            for idx, row in df.iterrows():
                question_text = ragas_data[idx]["question"]
                # Tronquer la question si trop longue
                if len(question_text) > 60:
                    question_text = question_text[:57] + "..."

                print(f"Question {idx + 1}: {question_text}")
                print("-" * 70)

                for metric in metrics:
                    metric_name = metric.name
                    if metric_name in df.columns:
                        score = row[metric_name]
                        # Emoji selon le score
                        if score >= 0.8:
                            emoji = "‚úÖ"
                        elif score >= 0.6:
                            emoji = "‚ö†Ô∏è "
                        else:
                            emoji = "‚ùå"
                        print(f"  {emoji} {metric_name:25s}: {score:.4f}")
                print("")

        # Afficher le rapport des moyennes
        print("=" * 70)
        print("üìà M√âTRIQUES RAGAS (MOYENNES)")
        print("=" * 70)
        print("")

        # Extraire les scores moyens
        scores = {}

        if df is not None:
            # Calculer les moyennes pour chaque m√©trique
            for metric in metrics:
                metric_name = metric.name
                if metric_name in df.columns:
                    score = df[metric_name].mean()
                    scores[metric_name] = score
                    # Afficher avec un emoji selon le score
                    if score >= 0.8:
                        emoji = "‚úÖ"
                    elif score >= 0.6:
                        emoji = "‚ö†Ô∏è "
                    else:
                        emoji = "‚ùå"
                    print(f"  {emoji} {metric_name:25s}: {score:.4f}")
        else:
            # Fallback : essayer d'acc√©der directement aux attributs
            for metric in metrics:
                metric_name = metric.name
                if hasattr(result, metric_name):
                    score = getattr(result, metric_name)
                    if isinstance(score, (int, float)):
                        scores[metric_name] = score
                        # Afficher avec un emoji selon le score
                        if score >= 0.8:
                            emoji = "‚úÖ"
                        elif score >= 0.6:
                            emoji = "‚ö†Ô∏è "
                        else:
                            emoji = "‚ùå"
                        print(f"  {emoji} {metric_name:25s}: {score:.4f}")

        print("")
        print("=" * 70)
        print("INTERPR√âTATION DES SCORES")
        print("=" * 70)
        print("")
        print("  ‚úÖ Faithfulness (Fid√©lit√©) [0-1]:")
        print("     Mesure si la r√©ponse est fid√®le au contexte r√©cup√©r√©")
        print("     > 0.8 = Excellent | 0.6-0.8 = Bon | < 0.6 = √Ä am√©liorer")
        print("")
        print("  ‚úÖ Answer Relevancy (Pertinence) [0-1]:")
        print("     Mesure la pertinence de la r√©ponse √† la question")
        print("     > 0.8 = Excellent | 0.6-0.8 = Bon | < 0.6 = √Ä am√©liorer")
        print("")
        print("  ‚úÖ Context Precision (Pr√©cision du contexte) [0-1]:")
        print("     Mesure la pr√©cision du contexte r√©cup√©r√©")
        print("     > 0.8 = Excellent | 0.6-0.8 = Bon | < 0.6 = √Ä am√©liorer")
        print("")
        print("  ‚úÖ Context Recall (Rappel du contexte) [0-1]:")
        print("     Mesure la compl√©tude du contexte r√©cup√©r√©")
        print("     > 0.8 = Excellent | 0.6-0.8 = Bon | < 0.6 = √Ä am√©liorer")
        print("")
        print("=" * 70)
        print("RECOMMANDATIONS")
        print("=" * 70)
        print("")

        # Analyser les r√©sultats et donner des recommandations
        if scores.get("faithfulness", 0) < 0.7:
            print("  ‚ö†Ô∏è  Faithfulness faible :")
            print("     - V√©rifiez que les r√©ponses restent fid√®les au contexte")
            print("     - Ajustez le prompt syst√®me pour √©viter les hallucinations")
            print("")

        if scores.get("answer_relevancy", 0) < 0.7:
            print("  ‚ö†Ô∏è  Answer Relevancy faible :")
            print("     - V√©rifiez que les r√©ponses adressent bien la question")
            print("     - Am√©liorez la qualit√© du prompt d'enrichissement")
            print("")

        if scores.get("context_precision", 0) < 0.7:
            print("  ‚ö†Ô∏è  Context Precision faible :")
            print("     - Am√©liorez la qualit√© des embeddings (mod√®le, chunking)")
            print("     - Ajustez les param√®tres de recherche (top_k, seuil)")
            print("")

        if scores.get("context_recall", 0) < 0.7:
            print("  ‚ö†Ô∏è  Context Recall faible :")
            print("     - Augmentez le nombre de contextes r√©cup√©r√©s (top_k)")
            print("     - V√©rifiez la compl√©tude de votre base de donn√©es")
            print("")

        # Si tous les scores sont bons
        if all(s >= 0.7 for s in scores.values()):
            print("  ‚úÖ Tous les scores sont satisfaisants !")
            print("     Votre syst√®me RAG fonctionne correctement.")
            print("")

        print("=" * 70)
        print("‚úÖ RAPPORT RAGAS TERMIN√â")
        print("=" * 70)

    except Exception as e:
        print(f"\n‚ùå Erreur lors de la g√©n√©ration du rapport RAGAS: {e}")
        import traceback
        traceback.print_exc()


# ============================================================================
# Point d'entr√©e principal
# ============================================================================

def main():
    """
    Point d'entr√©e principal du script d'√©valuation RAGAS.
    """
    # Parser les arguments en ligne de commande
    parser = argparse.ArgumentParser(
        description="√âvaluation RAGAS du syst√®me RAG",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples:
  # Utiliser le fichier par d√©faut (ragas_test_questions_collected.json)
  python tests/evaluate_ragas.py

  # Utiliser un fichier sp√©cifique
  python tests/evaluate_ragas.py tests/ragas_test_questions_collected.json
  python tests/evaluate_ragas.py tests/my_custom_questions.json
        """
    )
    parser.add_argument(
        "questions_file",
        nargs="?",
        default=None,
        help="Chemin vers le fichier de questions JSON (d√©faut: ragas_test_questions_collected.json)"
    )

    args = parser.parse_args()

    print("\n" + "=" * 70)
    print("üéØ √âVALUATION RAGAS DU SYST√àME RAG")
    print("=" * 70)
    print("\nPr√©requis:")
    print("  - API RAG d√©marr√©e: make run-api")
    print("  - Index FAISS cr√©√©: make run-embeddings")
    print("  - MISTRAL_API_KEY configur√©e dans .env")
    print("")

    # V√©rifier que l'API est accessible
    print("üîç V√©rification de l'API RAG...")
    if not check_api_health():
        print("\n‚ùå L'API RAG n'est pas accessible ou non fonctionnelle")
        print(f"   URL: {API_URL}")
        print("   Assurez-vous que l'API est d√©marr√©e avec 'make run-api'")
        sys.exit(1)

    print("‚úÖ API RAG accessible et fonctionnelle\n")

    # Charger les questions de test
    test_questions = load_test_questions(args.questions_file)
    if not test_questions:
        print("\n‚ùå Aucune question de test disponible. V√©rifiez le fichier JSON.")
        sys.exit(1)

    # Collecter les donn√©es
    ragas_data = collect_rag_data(test_questions)

    # G√©n√©rer le rapport
    if ragas_data:
        generate_ragas_report(ragas_data)
    else:
        print("\n‚ùå Aucune donn√©e collect√©e. Impossible de g√©n√©rer le rapport.")
        sys.exit(1)


if __name__ == "__main__":
    main()
