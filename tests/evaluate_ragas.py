"""
Script d'√©valuation RAGAS pour le syst√®me RAG.

Ce script √©value la qualit√© du RAG (Retrieval Augmented Generation) en utilisant
le framework RAGAS. Il g√©n√®re un rapport avec les m√©triques suivantes :
- Faithfulness : Fid√©lit√© de la r√©ponse au contexte
- Answer Relevancy : Pertinence de la r√©ponse √† la question
- Context Precision : Pr√©cision du contexte r√©cup√©r√©
- Context Recall : Compl√©tude du contexte r√©cup√©r√©

Pr√©requis :
- API FastAPI d√©marr√©e (make run-api)
- Index FAISS cr√©√© (make run-embeddings)
- Variables d'environnement configur√©es (.env et .env.test)

Usage:
    python tests/evaluate_ragas.py [fichier_questions.json]
    make test-ragas

Arguments:
    fichier_questions.json : Optionnel, chemin vers le fichier de questions
                             (d√©faut: ragas_data/ragas_test_questions_collected.json)
"""

import os
import sys
import time
import json
import argparse
import requests
from typing import List, Dict, Any
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv
from datasets import Dataset
from ragas import evaluate
from ragas.metrics import (
    faithfulness,
    answer_relevancy,
    context_precision,
    context_recall,
)
from langchain_mistralai import ChatMistralAI
from langchain_huggingface import HuggingFaceEmbeddings


# ============================================================================
# Configuration
# ============================================================================

# Charger les variables d'environnement
env_path = Path(__file__).parent.parent / ".env"
test_env_path = Path(__file__).parent.parent / ".env.test"

if env_path.exists():
    load_dotenv(env_path)
if test_env_path.exists():
    load_dotenv(test_env_path, override=True)

# Configuration
API_URL = os.getenv("RAG_API_URL", "http://localhost:8000")
RAGAS_TOP_K = int(os.getenv("RAGAS_TOP_K", "5"))
RAGAS_API_TIMEOUT = float(os.getenv("RAGAS_API_TIMEOUT", "30"))
RAGAS_MISTRAL_DELAY = float(os.getenv("RAGAS_MISTRAL_DELAY", "2.0"))
MISTRAL_API_KEY = os.getenv("MISTRAL_API_KEY")
MISTRAL_MODEL = os.getenv("MISTRAL_MODEL", "mistral-small-latest")
EMBEDDINGS_MODEL = os.getenv("EMBEDDINGS_MODEL", "intfloat/multilingual-e5-large")


# ============================================================================
# Dataset de test
# ============================================================================

def load_test_questions(json_path: str = None) -> List[Dict[str, Any]]:
    """
    Charge les cas de test depuis un fichier JSON.

    Par d√©faut, utilise UNIQUEMENT le fichier ragas_data/ragas_test_questions_collected.json.
    Si ce fichier n'existe pas, le script s'arr√™te avec une erreur explicite.

    Pour g√©n√©rer ce fichier: make collect-ragas

    Structure attendue du JSON:
    {
      "test_cases": [
        {
          "id": "test_001",
          "question": "...",
          "answer": "..." ou null,
          "contexts": [...] ou null,
          "ground_truth": "...",
          "category": "...",
          "location": "...",
          "notes": "..."
        }
      ]
    }

    Args:
        json_path : Chemin vers le fichier JSON (optionnel, utilise ragas_data/ragas_test_questions_collected.json si None)

    Returns:
        list : Liste de dictionnaires avec question, answer, contexts, ground_truth
    """
    if json_path is None:
        # Utiliser UNIQUEMENT le fichier collected
        collected_path = (
            Path(__file__).parent / "ragas_data" / "ragas_test_questions_collected.json"
        )

        if not collected_path.exists():
            print("\n‚ùå ERREUR: Fichier de donn√©es collect√©es introuvable")
            print(f"   Fichier attendu: {collected_path}")
            print("")
            print("üìã Pour g√©n√©rer ce fichier:")
            print("   1. D√©marrez l'API: make run-api")
            print("   2. Collectez les donn√©es: make collect-ragas")
            print("   3. Relancez l'√©valuation: make test-ragas")
            print("")
            sys.exit(1)

        json_path = collected_path
        print(f"üì¶ Utilisation du fichier pr√©-collect√©: {collected_path.name}")
    else:
        json_path = Path(json_path)

    try:
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)

        # Nouvelle structure: "test_cases" au lieu de "questions"
        test_cases = data.get("test_cases", [])
        if not test_cases:
            print(f"‚ö†Ô∏è  Aucun cas de test trouv√© dans {json_path}")
            return []

        print(f"üìã Charg√© {len(test_cases)} cas de test depuis {json_path.name}")
        if "description" in data:
            print(f"   Description: {data['description']}")
        if "version" in data:
            print(f"   Version: {data['version']}")

        # Compter combien ont d√©j√† answer/contexts pr√©-collect√©s
        pre_collected = sum(1 for tc in test_cases if tc.get("answer") and tc.get("contexts"))
        if pre_collected > 0:
            print(f"   ‚úì {pre_collected}/{len(test_cases)} cas avec answer/contexts pr√©-collect√©s")
        else:
            print("   ‚ö†Ô∏è  Aucun cas pr√©-collect√©. Les r√©ponses seront collect√©es dynamiquement.")

        return test_cases

    except FileNotFoundError:
        print(f"‚ùå Fichier non trouv√©: {json_path}")
        print("   Utilisation des cas de test par d√©faut")
        # Cas de test de fallback
        return [
            {
                "id": "fallback_001",
                "question": "Quels sont les √©v√©nements culturels gratuits √† Toulouse ?",
                "answer": None,
                "contexts": None,
                "ground_truth": "Il existe plusieurs √©v√©nements culturels gratuits √† Toulouse comme les concerts dans les parcs, les expositions municipales et les festivals de rue.",
            },
            {
                "id": "fallback_002",
                "question": "O√π puis-je trouver des expositions d'art contemporain en Occitanie ?",
                "answer": None,
                "contexts": None,
                "ground_truth": "Les expositions d'art contemporain en Occitanie sont disponibles dans plusieurs mus√©es et galeries √† Toulouse, Montpellier et dans d'autres villes de la r√©gion.",
            },
            {
                "id": "fallback_003",
                "question": "Quels festivals de musique ont lieu en √©t√© en Occitanie ?",
                "answer": None,
                "contexts": None,
                "ground_truth": "Plusieurs festivals de musique ont lieu en √©t√© en Occitanie, incluant des festivals de jazz, de musique classique et de musiques du monde.",
            },
        ]
    except json.JSONDecodeError as e:
        print(f"‚ùå Erreur lors du parsing JSON: {e}")
        print("   V√©rifiez la syntaxe du fichier JSON")
        sys.exit(1)
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement des cas de test: {e}")
        sys.exit(1)


# ============================================================================
# Fonctions utilitaires
# ============================================================================

def check_api_health() -> bool:
    """
    V√©rifie que l'API RAG est accessible et fonctionnelle.

    Returns:
        bool : True si l'API est accessible, False sinon
    """
    try:
        response = requests.get(f"{API_URL}/health", timeout=5)
        if response.status_code == 200:
            data = response.json()
            status_ok = data.get("status") in ["ok", "healthy"]
            vector_store_ok = data.get("vector_store_loaded", False)
            embeddings_ok = data.get("embeddings_model_loaded", False)
            return status_ok and vector_store_ok and embeddings_ok
        return False
    except requests.exceptions.RequestException as e:
        print(f"‚ùå Erreur lors du health check: {e}")
        return False


def format_contexts_for_ragas(context_used: List[Dict[str, Any]]) -> List[str]:
    """
    Formate les contextes r√©cup√©r√©s pour l'√©valuation RAGAS.

    Utilise le contenu de l'√©v√©nement pour l'√©valuation RAGAS.
    Cela permet √† RAGAS d'√©valuer la pertinence bas√©e sur le contenu
    textuel complet des √©v√©nements r√©cup√©r√©s.

    Args:
        context_used : Liste des contextes utilis√©s par le RAG

    Returns:
        list : Liste de contenus d'√©v√©nements (strings)
    """
    return [ctx.get("content", "") for ctx in context_used]


def validate_ragas_data(test_cases: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
    """
    Valide que les donn√©es RAGAS sont compl√®tes dans le fichier JSON.

    Cette fonction ne fait AUCUN appel API et ne lance PAS le RAG.
    Elle v√©rifie uniquement que chaque cas de test contient toutes les donn√©es:
    - question
    - answer (pr√©-collect√© via make collect-ragas)
    - contexts (pr√©-collect√©s via make collect-ragas)
    - ground_truth

    Si des donn√©es sont manquantes, le script s'arr√™te et affiche un message
    clair indiquant comment collecter les donn√©es manquantes.

    Args:
        test_cases : Liste des cas de test charg√©s depuis JSON

    Returns:
        list : Liste de dictionnaires valides avec question, answer, contexts, ground_truth

    Raises:
        SystemExit : Si des donn√©es manquantes ou invalides sont d√©tect√©es
    """
    print("\n" + "=" * 70)
    print("‚úÖ VALIDATION DES DONN√âES RAGAS")
    print("=" * 70)
    print(f"Nombre de cas de test √† valider: {len(test_cases)}\n")

    results = []
    valid_count = 0
    invalid_cases = []

    for i, test_case in enumerate(test_cases, 1):
        test_id = test_case.get("id", f"test_{i:03d}")
        question = test_case.get("question")
        answer = test_case.get("answer")
        contexts = test_case.get("contexts")
        ground_truth = test_case.get("ground_truth")

        print(f"[{i}/{len(test_cases)}] {test_id}")

        # V√©rifier que toutes les donn√©es requises sont pr√©sentes
        errors = []

        if not question:
            errors.append("question manquante")
        if not answer:
            errors.append("answer manquante")
        elif len(answer) < 50:
            errors.append(f"answer trop courte ({len(answer)} caract√®res)")
        if not contexts or len(contexts) == 0:
            errors.append("contexts manquants")
        if not ground_truth:
            errors.append("ground_truth manquant")

        if errors:
            print(f"   ‚ùå INVALIDE: {', '.join(errors)}")
            invalid_cases.append({
                "id": test_id,
                "errors": errors
            })
            continue

        # Formater les contextes si n√©cessaire
        if isinstance(contexts[0], dict):
            # Format API: list of dicts with 'content' key
            formatted_contexts = format_contexts_for_ragas(contexts)
        else:
            # Format RAGAS: list of strings (d√©j√† format√©)
            formatted_contexts = contexts

        results.append({
            "question": question,
            "answer": answer,
            "contexts": formatted_contexts,
            "ground_truth": ground_truth
        })

        print(f"   ‚úÖ VALIDE: {len(answer)} caract√®res, {len(formatted_contexts)} contextes")
        valid_count += 1

    print(f"\n{'=' * 70}")
    print("üìä R√âSULTAT DE LA VALIDATION")
    print("=" * 70)
    print(f"‚úÖ Cas valides: {valid_count}/{len(test_cases)}")
    print(f"‚ùå Cas invalides: {len(invalid_cases)}/{len(test_cases)}")

    if invalid_cases:
        print("\n‚ö†Ô∏è  CAS INVALIDES D√âTECT√âS:")
        for case in invalid_cases:
            print(f"   - {case['id']}: {', '.join(case['errors'])}")
        print("\nüí° Pour collecter les donn√©es manquantes:")
        print("   1. Assurez-vous que l'API RAG est lanc√©e: make run-api")
        print("   2. Lancez la collecte des donn√©es: make collect-ragas")
        print("   3. Relancez l'√©valuation: make test-ragas")
        print("=" * 70)
        sys.exit(1)

    print("=" * 70)

    return results


def generate_ragas_report(ragas_data: List[Dict[str, Any]]):
    """
    G√©n√®re le rapport final RAGAS avec les m√©triques calcul√©es.

    Args:
        ragas_data : Liste de dictionnaires avec question, answer, contexts, ground_truth
    """
    if not ragas_data:
        print("\n‚ö†Ô∏è  Aucune donn√©e √† √©valuer. Toutes les questions ont √©t√© ignor√©es.")
        print("   Causes possibles:")
        print("   - API Mistral a d√©pass√© son quota (429)")
        print("   - R√©ponses trop courtes ou contextes manquants")
        print("   - Probl√®me de connexion √† l'API")
        return

    print("\n" + "=" * 70)
    print("üéØ G√âN√âRATION DU RAPPORT RAGAS")
    print("=" * 70)

    try:
        # Cr√©er le dataset pour RAGAS
        dataset_dict = {
            "question": [r["question"] for r in ragas_data],
            "answer": [r["answer"] for r in ragas_data],
            "contexts": [r["contexts"] for r in ragas_data],
            "ground_truth": [r["ground_truth"] for r in ragas_data],
        }

        dataset = Dataset.from_dict(dataset_dict)

        print(f"\nüìä √âvaluation de {len(ragas_data)} questions...")
        print("‚è≥ Calcul des m√©triques RAGAS en cours...")
        print("   (Cette op√©ration peut prendre 30-60 secondes)")
        print("")

        # V√©rifier la cl√© API
        if not MISTRAL_API_KEY:
            print("‚ùå MISTRAL_API_KEY non configur√©e. Impossible d'√©valuer avec RAGAS.")
            return

        # Configurer le LLM Mistral AI pour RAGAS
        print(f"‚öôÔ∏è  Configuration LLM: model={MISTRAL_MODEL}, timeout={RAGAS_API_TIMEOUT}s, delay={RAGAS_MISTRAL_DELAY}s")
        print("   Note: RAGAS effectue plusieurs appels API pour calculer les m√©triques")
        print("   En cas d'erreur 429, augmentez RAGAS_MISTRAL_DELAY dans .env.test")
        print("")

        llm = ChatMistralAI(
            model=MISTRAL_MODEL,
            api_key=MISTRAL_API_KEY,
            temperature=0.0,
            max_retries=3,
            timeout=RAGAS_API_TIMEOUT
        )

        # Configurer les embeddings pour RAGAS (utilise E5 au lieu d'OpenAI)
        print(f"üì¶ Configuration embeddings: {EMBEDDINGS_MODEL}")
        embeddings = HuggingFaceEmbeddings(
            model_name=EMBEDDINGS_MODEL,
            model_kwargs={'device': 'cpu'},
            encode_kwargs={'normalize_embeddings': True}
        )

        # √âvaluer avec les m√©triques RAGAS
        metrics = [faithfulness, answer_relevancy, context_precision, context_recall]

        # D√©lai avant l'√©valuation pour r√©cup√©ration du quota
        if RAGAS_MISTRAL_DELAY > 0:
            print(f"‚è≥ Attente de {RAGAS_MISTRAL_DELAY}s avant d'√©valuer (r√©cup√©ration du quota API)...")
            time.sleep(RAGAS_MISTRAL_DELAY)

        result = evaluate(dataset, metrics=metrics, llm=llm, embeddings=embeddings)

        # Convertir en DataFrame si n√©cessaire
        df = None
        if hasattr(result, 'to_pandas'):
            df = result.to_pandas()

        # Afficher les scores d√©taill√©s par question
        if df is not None and len(df) > 0:
            print("\n" + "=" * 70)
            print("üìä SCORES D√âTAILL√âS PAR QUESTION")
            print("=" * 70)
            print("")

            for idx, row in df.iterrows():
                question_text = ragas_data[idx]["question"]
                # Tronquer la question si trop longue
                if len(question_text) > 60:
                    question_text = question_text[:57] + "..."

                print(f"Question {idx + 1}: {question_text}")
                print("-" * 70)

                for metric in metrics:
                    metric_name = metric.name
                    if metric_name in df.columns:
                        score = row[metric_name]
                        # Emoji selon le score
                        if score >= 0.8:
                            emoji = "‚úÖ"
                        elif score >= 0.6:
                            emoji = "‚ö†Ô∏è "
                        else:
                            emoji = "‚ùå"
                        print(f"  {emoji} {metric_name:25s}: {score:.4f}")
                print("")

        # Afficher le rapport des moyennes
        print("=" * 70)
        print("üìà M√âTRIQUES RAGAS (MOYENNES)")
        print("=" * 70)
        print("")

        # Extraire les scores moyens
        scores = {}

        if df is not None:
            # Calculer les moyennes pour chaque m√©trique
            for metric in metrics:
                metric_name = metric.name
                if metric_name in df.columns:
                    score = df[metric_name].mean()
                    scores[metric_name] = score
                    # Afficher avec un emoji selon le score
                    if score >= 0.8:
                        emoji = "‚úÖ"
                    elif score >= 0.6:
                        emoji = "‚ö†Ô∏è "
                    else:
                        emoji = "‚ùå"
                    print(f"  {emoji} {metric_name:25s}: {score:.4f}")
        else:
            # Fallback : essayer d'acc√©der directement aux attributs
            for metric in metrics:
                metric_name = metric.name
                if hasattr(result, metric_name):
                    score = getattr(result, metric_name)
                    if isinstance(score, (int, float)):
                        scores[metric_name] = score
                        # Afficher avec un emoji selon le score
                        if score >= 0.8:
                            emoji = "‚úÖ"
                        elif score >= 0.6:
                            emoji = "‚ö†Ô∏è "
                        else:
                            emoji = "‚ùå"
                        print(f"  {emoji} {metric_name:25s}: {score:.4f}")

        print("")
        print("=" * 70)
        print("INTERPR√âTATION DES SCORES")
        print("=" * 70)
        print("")
        print("  ‚úÖ Faithfulness (Fid√©lit√©) [0-1]:")
        print("     Mesure si la r√©ponse est fid√®le au contexte r√©cup√©r√©")
        print("     > 0.8 = Excellent | 0.6-0.8 = Bon | < 0.6 = √Ä am√©liorer")
        print("")
        print("  ‚úÖ Answer Relevancy (Pertinence) [0-1]:")
        print("     Mesure la pertinence de la r√©ponse √† la question")
        print("     > 0.8 = Excellent | 0.6-0.8 = Bon | < 0.6 = √Ä am√©liorer")
        print("")
        print("  ‚úÖ Context Precision (Pr√©cision du contexte) [0-1]:")
        print("     Mesure la pr√©cision du contexte r√©cup√©r√©")
        print("     > 0.8 = Excellent | 0.6-0.8 = Bon | < 0.6 = √Ä am√©liorer")
        print("")
        print("  ‚úÖ Context Recall (Rappel du contexte) [0-1]:")
        print("     Mesure la compl√©tude du contexte r√©cup√©r√©")
        print("     > 0.8 = Excellent | 0.6-0.8 = Bon | < 0.6 = √Ä am√©liorer")
        print("")
        print("=" * 70)
        print("RECOMMANDATIONS")
        print("=" * 70)
        print("")

        # Analyser les r√©sultats et donner des recommandations
        if scores.get("faithfulness", 0) < 0.7:
            print("  ‚ö†Ô∏è  Faithfulness faible :")
            print("     - V√©rifiez que les r√©ponses restent fid√®les au contexte")
            print("     - Ajustez le prompt syst√®me pour √©viter les hallucinations")
            print("")

        if scores.get("answer_relevancy", 0) < 0.7:
            print("  ‚ö†Ô∏è  Answer Relevancy faible :")
            print("     - V√©rifiez que les r√©ponses adressent bien la question")
            print("     - Am√©liorez la qualit√© du prompt d'enrichissement")
            print("")

        if scores.get("context_precision", 0) < 0.7:
            print("  ‚ö†Ô∏è  Context Precision faible :")
            print("     - Am√©liorez la qualit√© des embeddings (mod√®le, chunking)")
            print("     - Ajustez les param√®tres de recherche (top_k, seuil)")
            print("")

        if scores.get("context_recall", 0) < 0.7:
            print("  ‚ö†Ô∏è  Context Recall faible :")
            print("     - Augmentez le nombre de contextes r√©cup√©r√©s (top_k)")
            print("     - V√©rifiez la compl√©tude de votre base de donn√©es")
            print("")

        # Si tous les scores sont bons
        if all(s >= 0.7 for s in scores.values()):
            print("  ‚úÖ Tous les scores sont satisfaisants !")
            print("     Votre syst√®me RAG fonctionne correctement.")
            print("")

        print("=" * 70)
        print("‚úÖ RAPPORT RAGAS TERMIN√â")
        print("=" * 70)

        # G√©n√©rer le rapport HTML
        try:
            generate_html_report(ragas_data, df, scores, metrics)
        except Exception as html_error:
            print(f"\n‚ö†Ô∏è  Erreur lors de la g√©n√©ration du rapport HTML: {html_error}")
            import traceback
            traceback.print_exc()

    except Exception as e:
        print(f"\n‚ùå Erreur lors de la g√©n√©ration du rapport RAGAS: {e}")
        import traceback
        traceback.print_exc()


def generate_html_report(
    ragas_data: List[Dict[str, Any]],
    df,
    scores: Dict[str, float],
    metrics: list,
    output_path: str = "rapport/ragas/ragas_report.html"
) -> None:
    """
    G√©n√®re un rapport HTML des r√©sultats d'√©valuation RAGAS.

    Args:
        ragas_data: Liste des cas de test
        df: DataFrame avec les r√©sultats d√©taill√©s
        scores: Dictionnaire des scores moyens
        metrics: Liste des m√©triques √©valu√©es
        output_path: Chemin du fichier HTML de sortie
    """
    # Cr√©er le r√©pertoire de sortie si n√©cessaire
    output_dir = Path(output_path).parent
    output_dir.mkdir(parents=True, exist_ok=True)

    # G√©n√©rer le timestamp
    timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")

    # Template HTML
    html_content = f"""<!DOCTYPE html>
<html lang="fr">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Rapport d'√âvaluation RAGAS</title>
    <style>
        * {{
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }}

        body {{
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            line-height: 1.6;
            color: #333;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            padding: 20px;
        }}

        .container {{
            max-width: 1200px;
            margin: 0 auto;
            background: white;
            border-radius: 10px;
            box-shadow: 0 10px 40px rgba(0,0,0,0.2);
            overflow: hidden;
        }}

        .header {{
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 40px;
            text-align: center;
        }}

        .header h1 {{
            font-size: 2.5em;
            margin-bottom: 10px;
        }}

        .header .subtitle {{
            font-size: 1.1em;
            opacity: 0.9;
        }}

        .timestamp {{
            text-align: center;
            padding: 15px;
            background: #f8f9fa;
            color: #666;
            font-size: 0.9em;
        }}

        .content {{
            padding: 40px;
        }}

        .section {{
            margin-bottom: 40px;
        }}

        .section-title {{
            font-size: 1.8em;
            color: #667eea;
            margin-bottom: 20px;
            padding-bottom: 10px;
            border-bottom: 3px solid #667eea;
        }}

        .metrics-grid {{
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
            gap: 20px;
            margin-bottom: 30px;
        }}

        .metric-card {{
            background: #f8f9fa;
            border-radius: 10px;
            padding: 25px;
            text-align: center;
            transition: transform 0.3s ease, box-shadow 0.3s ease;
        }}

        .metric-card:hover {{
            transform: translateY(-5px);
            box-shadow: 0 5px 20px rgba(0,0,0,0.1);
        }}

        .metric-name {{
            font-size: 1em;
            color: #666;
            margin-bottom: 10px;
            text-transform: uppercase;
            letter-spacing: 1px;
        }}

        .metric-value {{
            font-size: 2.5em;
            font-weight: bold;
            margin-bottom: 10px;
        }}

        .metric-excellent {{
            color: #28a745;
        }}

        .metric-good {{
            color: #ffc107;
        }}

        .metric-poor {{
            color: #dc3545;
        }}

        .metric-bar {{
            height: 8px;
            background: #e9ecef;
            border-radius: 4px;
            overflow: hidden;
            margin-top: 10px;
        }}

        .metric-bar-fill {{
            height: 100%;
            transition: width 0.5s ease;
        }}

        .bar-excellent {{
            background: linear-gradient(90deg, #28a745, #20c997);
        }}

        .bar-good {{
            background: linear-gradient(90deg, #ffc107, #fd7e14);
        }}

        .bar-poor {{
            background: linear-gradient(90deg, #dc3545, #c82333);
        }}

        .questions-table {{
            width: 100%;
            border-collapse: collapse;
            margin-top: 20px;
            box-shadow: 0 2px 10px rgba(0,0,0,0.1);
        }}

        .questions-table th {{
            background: #667eea;
            color: white;
            padding: 15px;
            text-align: left;
            font-weight: 600;
        }}

        .questions-table td {{
            padding: 15px;
            border-bottom: 1px solid #e9ecef;
        }}

        .questions-table tr:hover {{
            background: #f8f9fa;
        }}

        .question-text {{
            font-weight: 500;
            color: #333;
            margin-bottom: 5px;
        }}

        .score-badge {{
            display: inline-block;
            padding: 4px 12px;
            border-radius: 20px;
            font-size: 0.85em;
            font-weight: 600;
        }}

        .badge-excellent {{
            background: #d4edda;
            color: #155724;
        }}

        .badge-good {{
            background: #fff3cd;
            color: #856404;
        }}

        .badge-poor {{
            background: #f8d7da;
            color: #721c24;
        }}

        .interpretation {{
            background: #e7f3ff;
            border-left: 4px solid #2196F3;
            padding: 20px;
            margin: 20px 0;
            border-radius: 5px;
        }}

        .interpretation h3 {{
            color: #2196F3;
            margin-bottom: 15px;
        }}

        .interpretation ul {{
            list-style: none;
            padding-left: 0;
        }}

        .interpretation li {{
            padding: 10px 0;
            border-bottom: 1px solid #cce5ff;
        }}

        .interpretation li:last-child {{
            border-bottom: none;
        }}

        .recommendations {{
            background: #fff3cd;
            border-left: 4px solid #ffc107;
            padding: 20px;
            margin: 20px 0;
            border-radius: 5px;
        }}

        .recommendations h3 {{
            color: #856404;
            margin-bottom: 15px;
        }}

        .recommendations ul {{
            padding-left: 20px;
        }}

        .recommendations li {{
            margin-bottom: 10px;
        }}

        .success-message {{
            background: #d4edda;
            border-left: 4px solid #28a745;
            color: #155724;
            padding: 20px;
            margin: 20px 0;
            border-radius: 5px;
            font-weight: 500;
        }}

        .footer {{
            background: #f8f9fa;
            padding: 20px;
            text-align: center;
            color: #666;
            font-size: 0.9em;
        }}

        @media print {{
            body {{
                background: white;
                padding: 0;
            }}

            .container {{
                box-shadow: none;
            }}

            .metric-card {{
                break-inside: avoid;
            }}
        }}
    </style>
</head>
<body>
    <div class="container">
        <div class="header">
            <h1>üìä Rapport d'√âvaluation RAGAS</h1>
            <div class="subtitle">Syst√®me RAG - √âv√©nements Culturels Occitanie</div>
        </div>

        <div class="timestamp">
            G√©n√©r√© le {timestamp}
        </div>

        <div class="content">
            <!-- M√©triques moyennes -->
            <div class="section">
                <h2 class="section-title">üìà M√©triques Moyennes</h2>
                <div class="metrics-grid">
"""

    # Ajouter les cartes de m√©triques
    for metric in metrics:
        metric_name = metric.name
        if metric_name in scores:
            score = scores[metric_name]
            score_percent = int(score * 100)

            # D√©terminer la classe CSS selon le score
            if score >= 0.8:
                value_class = "metric-excellent"
                bar_class = "bar-excellent"
                badge_class = "badge-excellent"
                badge_text = "Excellent"
            elif score >= 0.6:
                value_class = "metric-good"
                bar_class = "bar-good"
                badge_class = "badge-good"
                badge_text = "Bon"
            else:
                value_class = "metric-poor"
                bar_class = "bar-poor"
                badge_class = "badge-poor"
                badge_text = "√Ä am√©liorer"

            # Nom de m√©trique format√©
            metric_display = metric_name.replace("_", " ").title()

            html_content += f"""
                    <div class="metric-card">
                        <div class="metric-name">{metric_display}</div>
                        <div class="metric-value {value_class}">{score:.3f}</div>
                        <span class="score-badge {badge_class}">{badge_text}</span>
                        <div class="metric-bar">
                            <div class="metric-bar-fill {bar_class}" style="width: {score_percent}%"></div>
                        </div>
                    </div>
"""

    html_content += """
                </div>
            </div>

            <!-- Scores d√©taill√©s par question -->
            <div class="section">
                <h2 class="section-title">üìã Scores D√©taill√©s par Question</h2>
                <table class="questions-table">
                    <thead>
                        <tr>
                            <th style="width: 50px;">#</th>
                            <th>Question</th>
"""

    # En-t√™tes des m√©triques
    for metric in metrics:
        metric_display = metric.name.replace("_", " ").title()
        html_content += f"                            <th style=\"width: 120px; text-align: center;\">{metric_display}</th>\n"

    html_content += """
                        </tr>
                    </thead>
                    <tbody>
"""

    # Lignes de r√©sultats
    if df is not None and len(df) > 0:
        for idx, row in df.iterrows():
            question_text = ragas_data[idx]["question"]
            # Tronquer si n√©cessaire
            if len(question_text) > 100:
                question_text = question_text[:97] + "..."

            html_content += f"""
                        <tr>
                            <td style="text-align: center; font-weight: bold;">{idx + 1}</td>
                            <td><div class="question-text">{question_text}</div></td>
"""

            # Scores pour chaque m√©trique
            for metric in metrics:
                metric_name = metric.name
                if metric_name in df.columns:
                    score = row[metric_name]
                    score_str = f"{score:.3f}"

                    # Badge selon le score
                    if score >= 0.8:
                        badge_class = "badge-excellent"
                    elif score >= 0.6:
                        badge_class = "badge-good"
                    else:
                        badge_class = "badge-poor"

                    html_content += f"                            <td style=\"text-align: center;\"><span class=\"score-badge {badge_class}\">{score_str}</span></td>\n"
                else:
                    html_content += "                            <td style=\"text-align: center;\">N/A</td>\n"

            html_content += "                        </tr>\n"

    html_content += """
                    </tbody>
                </table>
            </div>

            <!-- Interpr√©tation -->
            <div class="section">
                <h2 class="section-title">üí° Interpr√©tation des Scores</h2>
                <div class="interpretation">
                    <h3>üìä Guide de lecture</h3>
                    <ul>
                        <li><strong>Faithfulness (Fid√©lit√©)</strong> : Mesure si la r√©ponse est fid√®le au contexte r√©cup√©r√©. > 0.8 = Excellent | 0.6-0.8 = Bon | < 0.6 = √Ä am√©liorer</li>
                        <li><strong>Answer Relevancy (Pertinence)</strong> : Mesure la pertinence de la r√©ponse √† la question. > 0.8 = Excellent | 0.6-0.8 = Bon | < 0.6 = √Ä am√©liorer</li>
                        <li><strong>Context Precision (Pr√©cision du contexte)</strong> : Mesure la pr√©cision du contexte r√©cup√©r√©. > 0.8 = Excellent | 0.6-0.8 = Bon | < 0.6 = √Ä am√©liorer</li>
                        <li><strong>Context Recall (Rappel du contexte)</strong> : Mesure la compl√©tude du contexte r√©cup√©r√©. > 0.8 = Excellent | 0.6-0.8 = Bon | < 0.6 = √Ä am√©liorer</li>
                    </ul>
                </div>
            </div>

            <!-- Recommandations -->
            <div class="section">
                <h2 class="section-title">üéØ Recommandations</h2>
"""

    # G√©n√©rer les recommandations bas√©es sur les scores
    has_recommendations = False

    if scores.get("faithfulness", 0) < 0.7:
        has_recommendations = True
        html_content += """
                <div class="recommendations">
                    <h3>‚ö†Ô∏è  Faithfulness faible</h3>
                    <ul>
                        <li>V√©rifiez que les r√©ponses restent fid√®les au contexte</li>
                        <li>Ajustez le prompt syst√®me pour √©viter les hallucinations</li>
                    </ul>
                </div>
"""

    if scores.get("answer_relevancy", 0) < 0.7:
        has_recommendations = True
        html_content += """
                <div class="recommendations">
                    <h3>‚ö†Ô∏è  Answer Relevancy faible</h3>
                    <ul>
                        <li>V√©rifiez que les r√©ponses adressent bien la question</li>
                        <li>Am√©liorez la qualit√© du prompt d'enrichissement</li>
                    </ul>
                </div>
"""

    if scores.get("context_precision", 0) < 0.7:
        has_recommendations = True
        html_content += """
                <div class="recommendations">
                    <h3>‚ö†Ô∏è  Context Precision faible</h3>
                    <ul>
                        <li>Am√©liorez la qualit√© des embeddings (mod√®le, chunking)</li>
                        <li>Ajustez les param√®tres de recherche (top_k, seuil)</li>
                    </ul>
                </div>
"""

    if scores.get("context_recall", 0) < 0.7:
        has_recommendations = True
        html_content += """
                <div class="recommendations">
                    <h3>‚ö†Ô∏è  Context Recall faible</h3>
                    <ul>
                        <li>Augmentez le nombre de contextes r√©cup√©r√©s (top_k)</li>
                        <li>V√©rifiez la compl√©tude de votre base de donn√©es</li>
                    </ul>
                </div>
"""

    if not has_recommendations:
        html_content += """
                <div class="success-message">
                    ‚úÖ Tous les scores sont satisfaisants ! Votre syst√®me RAG fonctionne correctement.
                </div>
"""

    html_content += """
            </div>
        </div>

        <div class="footer">
            <p>Rapport g√©n√©r√© automatiquement par le syst√®me d'√©valuation RAGAS</p>
            <p>Projet OpenClassrooms - √âv√©nements Culturels Occitanie</p>
        </div>
    </div>
</body>
</html>
"""

    # √âcrire le fichier HTML
    with open(output_path, 'w', encoding='utf-8') as f:
        f.write(html_content)

    print(f"\nüìÑ Rapport HTML g√©n√©r√©: {output_path}")


# ============================================================================
# Point d'entr√©e principal
# ============================================================================

def main():
    """
    Point d'entr√©e principal du script d'√©valuation RAGAS.
    """
    # Parser les arguments en ligne de commande
    parser = argparse.ArgumentParser(
        description="√âvaluation RAGAS du syst√®me RAG",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples:
  # Utiliser le fichier par d√©faut (ragas_data/ragas_test_questions_collected.json)
  python tests/evaluate_ragas.py

  # Utiliser un fichier sp√©cifique
  python tests/evaluate_ragas.py tests/ragas_data/ragas_test_questions_collected.json
  python tests/evaluate_ragas.py tests/ragas_data/my_custom_questions.json
        """
    )
    parser.add_argument(
        "questions_file",
        nargs="?",
        default=None,
        help="Chemin vers le fichier de questions JSON (d√©faut: ragas_data/ragas_test_questions_collected.json)"
    )

    args = parser.parse_args()

    print("\n" + "=" * 70)
    print("üéØ √âVALUATION RAGAS DU SYST√àME RAG")
    print("=" * 70)
    print("\nPr√©requis:")
    print("  - API RAG d√©marr√©e: make run-api")
    print("  - Index FAISS cr√©√©: make run-embeddings")
    print("  - MISTRAL_API_KEY configur√©e dans .env")
    print("")

    # V√©rifier que l'API est accessible
    print("üîç V√©rification de l'API RAG...")
    if not check_api_health():
        print("\n‚ùå L'API RAG n'est pas accessible ou non fonctionnelle")
        print(f"   URL: {API_URL}")
        print("   Assurez-vous que l'API est d√©marr√©e avec 'make run-api'")
        sys.exit(1)

    print("‚úÖ API RAG accessible et fonctionnelle\n")

    # Charger les questions de test
    test_questions = load_test_questions(args.questions_file)
    if not test_questions:
        print("\n‚ùå Aucune question de test disponible. V√©rifiez le fichier JSON.")
        sys.exit(1)

    # Collecter les donn√©es
    ragas_data = validate_ragas_data(test_questions)

    # G√©n√©rer le rapport
    if ragas_data:
        generate_ragas_report(ragas_data)
    else:
        print("\n‚ùå Aucune donn√©e collect√©e. Impossible de g√©n√©rer le rapport.")
        sys.exit(1)


if __name__ == "__main__":
    main()
